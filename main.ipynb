{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code is a modification of the Yenni et al. (2012) analysis:\n",
    "#### - has the option to keep the filter S1 >= 1 & S2 >= 1 or remove it\n",
    "#### - does not truncate the values\n",
    "#### - considers extinction N<1e-6 rather than N<1\n",
    "#### - includes Cushing et al. (2004) analytical results\n",
    "\n",
    "#### their original code: https://github.com/gmyenni/RareStabilizationSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyN_function.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_equation(r1, r2, a11, a12, a21, a22, solver='dynamics'):\n",
    "    if solver == 'analyN': # Analytical approximation for equilibrium populations\n",
    "        denominator1 = a11 - (a21 * a12) / a22\n",
    "        denominator2 = a22 - (a21 * a12) / a11\n",
    "        N1 = (r1 - 1 - (a12 / a22) * (r2 - 1)) / denominator1 if denominator1 != 0 else np.nan\n",
    "        N2 = (r2 - 1 - (a21 / a11) * (r1 - 1)) / denominator2 if denominator2 != 0 else np.nan\n",
    "        if np.isinf(N1) or np.isinf(N2) or np.isnan(N1) or np.isnan(N2):\n",
    "            initialNsp1 = 0\n",
    "            initialNsp2 = 0\n",
    "            N = np.zeros((100, 2))\n",
    "            N[0, :] = [initialNsp1, initialNsp2]   \n",
    "            for i in range(1, 100):\n",
    "                N[i, 0] = max((r1 - 1 - a12 * N[i-1, 1]) / a11, 0)\n",
    "                N[i, 1] = max((r2 - 1 - a21 * N[i-1, 0]) / a22, 0)\n",
    "            N1 = np.mean(N[:, 0])\n",
    "            N2 = np.mean(N[:, 1])\n",
    "        if N1 < 0 and N2 >= 0:\n",
    "            N1, N2 = 0.0, (r2 - 1) / a22 if a22 != 0 else 0.0\n",
    "        elif N2 < 0 and N1 >= 0:\n",
    "            N1, N2 = (r1 - 1) / a11 if a11 != 0 else 0.0, 0.0\n",
    "        elif N1 < 0 and N2 < 0:\n",
    "            N1, N2 = 0.0, 0.0\n",
    "        return N1, N2\n",
    "    elif solver == 'dynamics': # Numerical simulation\n",
    "        y1 = np.array([5.0], dtype=np.float64)\n",
    "        y2 = np.array([5.0], dtype=np.float64)\n",
    "        stop_run = False\n",
    "        i = 0\n",
    "        while not stop_run and i < 10000:\n",
    "            denom1 = 1 + a11 * y1[i] + a12 * y2[i]\n",
    "            denom2 = 1 + a22 * y2[i] + a21 * y1[i]\n",
    "            per_cap1 = r1 / denom1\n",
    "            per_cap2 = r2 / denom2\n",
    "            new_y1 = y1[i] * per_cap1\n",
    "            new_y2 = y2[i] * per_cap2\n",
    "            y1 = np.append(y1, new_y1)\n",
    "            y2 = np.append(y2, new_y2)\n",
    "            if i >= 1:\n",
    "                if (abs(y1[-1] - y1[-2]) < 1e-6 and abs(y2[-1] - y2[-2]) < 1e-6):\n",
    "                    stop_run = True\n",
    "            i += 1\n",
    "        return y1[-1], y2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_counts_test(filtered_data, print_on=False):\n",
    "    print('\\nPGR Statistics:\\n')\n",
    "    count_PGR1, count_PGR2 = [], []\n",
    "    for _, row in filtered_data.iterrows():\n",
    "        PGR1, PGR2 = getPCG(row['r1'], row['r2'], row['a11'], row['a12'], row['a21'], row['a22'], row['N1'], row['N2'])\n",
    "        count_PGR1.append(PGR1)\n",
    "        count_PGR2.append(PGR2)\n",
    "    count_PGR1 = pd.Series(count_PGR1).dropna()\n",
    "    count_PGR2 = pd.Series(count_PGR2).dropna()\n",
    "    if count_PGR1.empty or count_PGR2.empty:\n",
    "        print(\"No valid PGR data.\")\n",
    "        return None\n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        \"mean_PGR1\": count_PGR1.mean(),\n",
    "        \"mean_PGR2\": count_PGR2.mean(),\n",
    "        \"std_PGR1\": count_PGR1.std(ddof=1),\n",
    "        \"std_PGR2\": count_PGR2.std(ddof=1),\n",
    "        \"median_PGR1\": count_PGR1.median(),\n",
    "        \"median_PGR2\": count_PGR2.median()\n",
    "    }\n",
    "    print(f\"PGR1 Mean \\u00B1 SD: {stats['mean_PGR1']:.2g} \\u00B1 {stats['std_PGR1']:.2g}\")\n",
    "    print(f\"PGR2 Mean \\u00B1 SD: {stats['mean_PGR2']:.2g} \\u00B1 {stats['std_PGR2']:.2g}\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getNFD.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def SOS(r1, r2, a11, a12, a21, a22):\n",
    "    S1 = r2 / (1 + (a12 / a22) * (r2 - 1))\n",
    "    S2 = r1 / (1 + (a21 / a11) * (r1 - 1))\n",
    "    return S1, S2\n",
    "\n",
    "@jit\n",
    "def getPCG(r1, r2, a11, a12, a21, a22, N1, N2): # Per capita growth rate calculation\n",
    "    newN1 = r1 * N1 / (1 + a11 * N1 + a12 * N2) if N1 > 0 else np.nan\n",
    "    newN2 = r2 * N2 / (1 + a22 * N2 + a21 * N1) if N2 > 0 else np.nan\n",
    "    PGR1 = np.log(newN1) - np.log(N1) if N1 > 0 else np.nan\n",
    "    PGR2 = np.log(newN2) - np.log(N2) if N2 > 0 else np.nan\n",
    "    return PGR1, PGR2\n",
    "\n",
    "@jit\n",
    "def calculate_metrics(r1, r2, a11, a12, a21, a22, N1, N2, extinc_crit_1=True):\n",
    "    S1, S2 = SOS(r1, r2, a11, a12, a21, a22) # Strength of Stabilization\n",
    "    FE1, FE2 = r1 / r2, r2 / r1 # Fitness equivalence\n",
    "    Asy = S1 - S2 # Asymmetry\n",
    "    Rare = 0 if N1 == 0 and N2 == 0 else N1 / (N1 + N2)\n",
    "    # Calculating covariance for SoS\n",
    "    x = np.array([N1, N2])\n",
    "    y_sos = np.array([S1, S2])\n",
    "    cor_matrix_sos = np.cov(x, y_sos)\n",
    "    cor_sos = cor_matrix_sos[0, 1] # Extracting the correlation between N and SoS\n",
    "    Rank = 0 if N1 == 0 and N2 == 0 else (2 if N1 / (N1 + N2) <= 0.25 else 1)\n",
    "    # Equilibrium points\n",
    "    E1 = (r1 - 1) / a11\n",
    "    E2 = (r2 - 1) / a22\n",
    "    P = (r1 - 1) / a12\n",
    "    Q = (r2 - 1) / a21\n",
    "    # Calculate conditions for A, B, C, D\n",
    "    A = P > E2 and E1 > Q\n",
    "    B = E2 > P and Q > E1\n",
    "    C = P > E2 and Q > E1\n",
    "    D = E2 > P and E1 > Q\n",
    "    # Call getPCG to calculate PGR1 and PGR2\n",
    "    PGR1, PGR2 = getPCG(r1, r2, a11, a12, a21, a22, N1, N2)\n",
    "    if extinc_crit_1:\n",
    "        Coexist = 0 if N1 < 1 or N2 < 1 else 1\n",
    "    else:\n",
    "        Coexist = 0 if N1 < 1.0e-6 or N2 < 1.0e-6 else 1\n",
    "    return {\"FE1\": FE1, \"S1\": S1, \"FE2\": FE2, \"S2\": S2, \"Rank\": Rank, \"Coexist\": Coexist, \"Asy\": Asy, \"cor_sos\": cor_sos, \"Rare\": Rare, \"PGR1\": PGR1, \"PGR2\": PGR2, \"A\": A, \"B\": B, \"C\": C, \"D\": D}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# annualplant_2spp_det_par.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(pars):\n",
    "    # Defines frequency-dependent parameters\n",
    "    if pars == 'r_code': # Their R code\n",
    "         r1_v = np.arange(10, 21, 1)\n",
    "         r2_v = np.arange(10, 21, 1)\n",
    "         a11_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1, 1.5, 2, 2.5, 3])\n",
    "         a12_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "         a21_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "         a22_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "    elif pars == 'table1': # Reproduce their Table 1\n",
    "        r1_v = np.arange(15, 21, 1)\n",
    "        r2_v = np.arange(15, 21, 1)\n",
    "        a11_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1, 1.5, 2, 2.5, 3])\n",
    "        a12_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "        a21_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "        a22_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "    elif pars == 'paper': # They describe in the paper\n",
    "         r1_v = np.arange(15, 21, 1)\n",
    "         r2_v = np.arange(11, 21, 1)\n",
    "         a11_v = np.array([0.7, 0.3, 0.5, 0.7, 0.9, 1, 1.5, 2, 2.5, 3])\n",
    "         a12_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "         a21_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "         a22_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "    else: # minimal: Reduced set of parameters\n",
    "        r1_v = np.array([15, 17, 18, 20])\n",
    "        r2_v = np.array([15, 17, 18, 20])\n",
    "        a11_v = np.array([0.1, 1, 3])\n",
    "        a12_v = np.array([0.1, 0.5, 1])\n",
    "        a21_v = np.array([0.1, 0.5, 1])\n",
    "        a22_v = np.array([0.1, 0.5, 1])\n",
    "    # Generate all combinations of parameters using NumPy's meshgrid\n",
    "    mesh = np.array(np.meshgrid(r1_v, r2_v, a11_v, a12_v, a21_v, a22_v)).T.reshape(-1, 6)\n",
    "    return mesh\n",
    "\n",
    "def Sim(k, mesh_row, extinc_crit_1=False, solver='dynamics'):\n",
    "    start_time = time.time()\n",
    "    r1, r2, a11, a12, a21, a22 = mesh_row\n",
    "    N1, N2 = difference_equation(r1, r2, a11, a12, a21, a22, solver=solver)\n",
    "    metrics = calculate_metrics(r1, r2, a11, a12, a21, a22, N1, N2, extinc_crit_1)\n",
    "    execution_time = time.time() - start_time\n",
    "    return {**metrics, \"N1\": N1, \"N2\": N2, \"r1\": r1, \"r2\": r2, \"a11\": a11, \"a12\": a12, \"a21\": a21, \"a22\": a22}\n",
    "\n",
    "def postprocess_results(results, outfile):\n",
    "    column_order = ['r1', 'r2', 'a11', 'a12', 'a21', 'a22', 'N1', 'N2', 'FE1', 'S1', 'FE2', 'S2', 'Rank', 'Coexist', 'Asy', 'cor_sos', 'Rare', 'PGR1', 'PGR2', 'A', 'B', 'C', 'D']\n",
    "    simul = pd.DataFrame(results, columns=column_order)\n",
    "    simul.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cor_figure.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_figure(filter, truncate=False):\n",
    "    dat_det = pd.read_csv(\"csv/annplant_2spp_det_rare.csv\")\n",
    "    if filter == 'inverted':\n",
    "        dat_det = dat_det.query('Rank == 2 & S1 < 1 & S2 < 1').copy()\n",
    "    elif filter == 'on':\n",
    "        dat_det = dat_det.query('Rank == 2 & S1 >= 1 & S2 >= 1').copy()\n",
    "    else: # 'off'\n",
    "        dat_det = dat_det.query('Rank == 2').copy()\n",
    "    dat_det.reset_index(drop=True, inplace=True)\n",
    "    if truncate:\n",
    "        dat_det = np.trunc(dat_det * 100) / 100.0\n",
    "    dat_det.sort_values(by=['a22', 'a21', 'a12', 'a11', 'r2', 'r1'], inplace=True)\n",
    "    dat_det.to_csv(f\"csv/annplant_2spp_det_rare_filtered_{filter}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figures_det.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_logistic_regression(dat, analysis_type, print_on=False):\n",
    "    predictors_map = {\n",
    "        'SoS': ['S1', 'FE1', 'cor_sos'],\n",
    "    }    \n",
    "    predictors = predictors_map[analysis_type]\n",
    "    X = sm.add_constant(dat[predictors])\n",
    "    y = dat['Coexist']\n",
    "    model = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "    result = model.fit()\n",
    "    print(result.summary())\n",
    "    coef = result.params\n",
    "    std_err = result.bse\n",
    "    z_scores = result.tvalues\n",
    "    p_values = result.pvalues\n",
    "    intercept = coef[0]\n",
    "    coef = coef[1:]\n",
    "    if print_on: # same analysis in more detail\n",
    "        print(\"\\n\\n--------------------------------------------------------\\n\\n\")\n",
    "        result_table = result.summary2().tables[1]\n",
    "        # Apply maximum precision to coefficient-related statistics\n",
    "        result_table['Coef.'] = result_table['Coef.']\n",
    "        result_table['Std.Err.'] = result_table['Std.Err.']\n",
    "        result_table['z'] = result_table['z'].apply(lambda x: np.format_float_scientific(x, precision=4))\n",
    "        result_table['P>|z|'] = result_table['P>|z|'].apply(lambda x: np.format_float_scientific(x, precision=4))\n",
    "        result_table = result_table.round(4)\n",
    "        print(f\"\\n{analysis_type} Analysis (in more detail):\")\n",
    "        print(result_table)\n",
    "    return intercept, coef, std_err, z_scores, p_values\n",
    "\n",
    "def calculate_proportions(dat, correlation_type):\n",
    "    proportions = {}\n",
    "    for cor_type in [correlation_type]:\n",
    "        proportions[f'positive_coexistence_{cor_type}'] = len(dat[(dat[cor_type] >= 0) & (dat['Coexist'] == 1)])\n",
    "        proportions[f'positive_exclusion_{cor_type}'] = len(dat[(dat[cor_type] >= 0) & (dat['Coexist'] == 0)])\n",
    "        proportions[f'negative_coexistence_{cor_type}'] = len(dat[(dat[cor_type] < 0) & (dat['Coexist'] == 1)])\n",
    "        proportions[f'negative_exclusion_{cor_type}'] = len(dat[(dat[cor_type] < 0) & (dat['Coexist'] == 0)])\n",
    "    return proportions\n",
    "\n",
    "def report_coexistence_analysis(proportions, correlation_type):\n",
    "    positive_key = f'positive_coexistence_{correlation_type}'\n",
    "    negative_key = f'negative_coexistence_{correlation_type}'\n",
    "    neg_confint = proportion_confint(count=proportions[negative_key], nobs=proportions[negative_key] + proportions[f'negative_exclusion_{correlation_type}'], alpha=0.05, method='wilson')\n",
    "    pos_confint = proportion_confint(count=proportions[positive_key], nobs=proportions[positive_key] + proportions[f'positive_exclusion_{correlation_type}'], alpha=0.05, method='wilson')\n",
    "    print(f\"\\nAnalysis on Negative \\u03BD for {correlation_type.upper()}:\")\n",
    "    print(f\"Proportion of coexistence with \\u03BD \\u2265 0: {proportions[positive_key] / (proportions[positive_key] + proportions[f'positive_exclusion_{correlation_type}']):.2g} (95% CI: {pos_confint})\")\n",
    "    print(f\"Proportion of coexistence with \\u03BD < 0: {proportions[negative_key] / (proportions[negative_key] + proportions[f'negative_exclusion_{correlation_type}']):.2g} (95% CI: {neg_confint})\")\n",
    "    \n",
    "def analyze_coexistence_effect(data, print_on):\n",
    "    original_dat = data.copy()\n",
    "    models_results = {}\n",
    "    for correlation_type in ['SoS']:\n",
    "        analysis_type = f'{correlation_type}'\n",
    "        correlation_column = 'cor_sos'\n",
    "        if correlation_column not in data.columns:\n",
    "            continue\n",
    "        print(f\"\\n--- Analysis for {analysis_type} ---\")\n",
    "        intercept, coef, std_err, z_scores, p_values = perform_logistic_regression(data, analysis_type, print_on=print_on)\n",
    "        models_results[analysis_type] = {\n",
    "            'statsmodels': (intercept, coef, std_err, z_scores, p_values),\n",
    "        }\n",
    "        proportions = calculate_proportions(data, correlation_column)\n",
    "        report_coexistence_analysis(proportions, correlation_column)\n",
    "        table_data = {\n",
    "            '\\u03BD \\u2265 0': [proportions[f'positive_coexistence_{correlation_column}'], proportions[f'positive_exclusion_{correlation_column}']],\n",
    "            '\\u03BD < 0': [proportions[f'negative_coexistence_{correlation_column}'], proportions[f'negative_exclusion_{correlation_column}']]\n",
    "        }\n",
    "        table_df = pd.DataFrame(table_data, index=['Coexistence', 'Exclusion'])\n",
    "        print(f\"Coexistence and Exclusion based on \\u03BD for {analysis_type}:\\n\", table_df)\n",
    "        # Proportion of coexistence calculations and confidence intervals\n",
    "        pos_confint = proportion_confint(count=proportions[f'positive_coexistence_{correlation_column}'], nobs=proportions[f'positive_coexistence_{correlation_column}'] + proportions[f'positive_exclusion_{correlation_column}'], alpha=0.05, method='wilson')\n",
    "        neg_confint = proportion_confint(count=proportions[f'negative_coexistence_{correlation_column}'], nobs=proportions[f'negative_coexistence_{correlation_column}'] + proportions[f'negative_exclusion_{correlation_column}'], alpha=0.05, method='wilson')\n",
    "        # Decision making based on confidence intervals\n",
    "        if neg_confint[1] >= pos_confint[0] and neg_confint[0] <= pos_confint[1]:  # Overlap\n",
    "            print(f\"The confidence intervals overlap for {analysis_type}, indicating they are statistically the same, not supporting the authors' results.\")\n",
    "        elif neg_confint[1] > pos_confint[0]:  # Negative larger than positive\n",
    "            print(f\"Higher coexistence observed with \\u03BD < 0 for {analysis_type}, supporting the authors' results.\")\n",
    "        else:  # Negative smaller than positive\n",
    "            print(f\"Higher coexistence observed with \\u03BD \\u2265 0 for {analysis_type}, not supporting the authors' results.\")\n",
    "    return models_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_abcd(filtered_data):\n",
    "    # Initialize counters\n",
    "    count_A_0, count_A_1 = 0, 0\n",
    "    count_B_0, count_B_1 = 0, 0\n",
    "    count_C_0, count_C_1 = 0, 0\n",
    "    count_D_0, count_D_1 = 0, 0\n",
    "    # Loop over the filtered dataset and apply the conditions for A, B, C, D\n",
    "    for index, row in filtered_data.iterrows():\n",
    "        P = (row['r1'] - 1) / row['a12']\n",
    "        E2 = (row['r2'] - 1) / row['a22']\n",
    "        Q = (row['r2'] - 1) / row['a21']\n",
    "        E1 = (row['r1'] - 1) / row['a11']\n",
    "        if P != E2 and Q != E1:\n",
    "            if P > E2 and E1 > Q:  # Case A\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_A_0 += 1\n",
    "                else:\n",
    "                    count_A_1 += 1\n",
    "            elif E2 > P and Q > E1:  # Case B\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_B_0 += 1\n",
    "                else:\n",
    "                    count_B_1 += 1\n",
    "            elif P > E2 and Q > E1:  # Case C\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_C_0 += 1\n",
    "                else:\n",
    "                    count_C_1 += 1\n",
    "            elif E2 > P and E1 > Q:  # Case D\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_D_0 += 1\n",
    "                else:\n",
    "                    count_D_1 += 1\n",
    "        elif P == E2 and Q == E1:\n",
    "            if row['Coexist'] == 0:\n",
    "                count_C_0 += 1\n",
    "            else:\n",
    "                count_C_1 += 1\n",
    "        elif P == E2:  # If P equals E2, only look at the relationship between Q and E1\n",
    "            if Q > E1:\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_B_0 += 1\n",
    "                else:\n",
    "                    count_B_1 += 1\n",
    "            elif E1 > Q:\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_A_0 += 1\n",
    "                else:\n",
    "                    count_A_1 += 1\n",
    "        elif Q == E1:  # If Q equals E1, only look at the relationship between P and E2\n",
    "            if P > E2:\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_A_0 += 1\n",
    "                else:\n",
    "                    count_A_1 += 1\n",
    "            elif E2 > P:\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_B_0 += 1\n",
    "                else:\n",
    "                    count_B_1 += 1\n",
    "    # Calculate totals for A, B, C, D\n",
    "    count_A_total = count_A_0 + count_A_1\n",
    "    count_B_total = count_B_0 + count_B_1\n",
    "    count_C_total = count_C_0 + count_C_1\n",
    "    count_D_total = count_D_0 + count_D_1\n",
    "    total_count = len(filtered_data)\n",
    "    # Calculate proportions\n",
    "    prop_A = count_A_total / total_count if total_count != 0 else 0\n",
    "    prop_B = count_B_total / total_count if total_count != 0 else 0\n",
    "    prop_C = count_C_total / total_count if total_count != 0 else 0\n",
    "    prop_D = count_D_total / total_count if total_count != 0 else 0\n",
    "    # Print results\n",
    "    print(f\"A\\nCoexist==0: {count_A_0}\\nCoexist==1: {count_A_1}\\nTotal: {count_A_total}\\nProportion: {prop_A:.2g}\")\n",
    "    print(f\"\\nB\\nCoexist==0: {count_B_0}\\nCoexist==1: {count_B_1}\\nTotal: {count_B_total}\\nProportion: {prop_B:.2g}\")\n",
    "    print(f\"\\nC\\nCoexist==0: {count_C_0}\\nCoexist==1: {count_C_1}\\nTotal: {count_C_total}\\nProportion: {prop_C:.2g}\")\n",
    "    print(f\"\\nD\\nCoexist==0: {count_D_0}\\nCoexist==1: {count_D_1}\\nTotal: {count_D_total}\\nProportion: {prop_D:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phase_plane():\n",
    "    # Parameters for each scenario\n",
    "    scenarios = {\n",
    "        \"A: $E_1 > Q$ and $P > E_2$\": {'r1': 18, 'r2': 16, 'a11': 0.5, 'a12': 1, 'a21': 1, 'a22': 1},\n",
    "        \"B: $Q > E_1$ and $E_2 > P$\": {'r1': 20, 'r2': 15, 'a11': 2, 'a12': 1, 'a21': 1, 'a22': 0.5},\n",
    "        \"C: $Q > E_1$ and $P > E_2$\":  {'r1': 20, 'r2': 15, 'a11': 3, 'a12': 0.5, 'a21': 1, 'a22': 0.5},\n",
    "        \"D: $E_1 > Q$ and $E_2 > P$\":  {'r1': 16, 'r2': 18, 'a11': 0.3, 'a12': 1, 'a21': 1, 'a22': 0.5},\n",
    "    }    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()    \n",
    "    for i, (title, params) in enumerate(scenarios.items()):\n",
    "        ax = axes[i]\n",
    "        # Unpack parameters\n",
    "        r1 = params['r1']\n",
    "        r2 = params['r2']\n",
    "        a11 = params['a11']\n",
    "        a12 = params['a12']\n",
    "        a21 = params['a21']\n",
    "        a22 = params['a22']\n",
    "        # Equilibrium points\n",
    "        E1 = [(r1 - 1) / a11, 0]\n",
    "        Q = [(r2 - 1) / a21, 0]\n",
    "        E2 = [0, (r2 - 1) / a22]\n",
    "        P = [0, (r1 - 1) / a12]\n",
    "        E0 = [0, 0]\n",
    "        # Calculate the intersection point of lines (E1, Q) and (E2, P)\n",
    "        a1 = (P[1] - E1[1]) / (P[0] - E1[0]) if P[0] != E1[0] else float('inf')\n",
    "        b1 = E1[1] - a1 * E1[0]\n",
    "        a2 = (E2[1] - Q[1]) / (E2[0] - Q[0]) if E2[0] != Q[0] else float('inf')\n",
    "        b2 = Q[1] - a2 * Q[0]\n",
    "        if a1 != a2:  # Ensure lines are not parallel\n",
    "            E3_x = (b2 - b1) / (a1 - a2) if a1 != float('inf') and a2 != float('inf') else 0\n",
    "            E3_y = a1 * E3_x + b1 if a1 != float('inf') else a2 * E3_x + b2\n",
    "            E3 = [E3_x, E3_y]\n",
    "        else:\n",
    "            E3 = None\n",
    "        # Extend axis limits by 10%\n",
    "        max_N1 = max(E1[0], Q[0])\n",
    "        max_N2 = max(E2[1], P[1])\n",
    "        N1 = np.linspace(0, max_N1, 30)\n",
    "        N2 = np.linspace(0, max_N2, 30)\n",
    "        N1, N2 = np.meshgrid(N1, N2)\n",
    "        # Compute the discrete system\n",
    "        N1_next = r1 * N1 / (1 + a11 * N1 + a12 * N2)\n",
    "        N2_next = r2 * N2 / (1 + a22 * N2 + a21 * N1)\n",
    "        # Plot vector field\n",
    "        ax.quiver(N1, N2, N1_next - N1, N2_next - N2, angles='xy', scale_units='xy', scale=15, color='grey', alpha=1)\n",
    "        # Plot equilibrium points\n",
    "        ax.plot(E0[0], E0[1], 'ko', label='E0', markersize=8)\n",
    "        ax.plot(E1[0], E1[1], 'bo', label='E1', markersize=8)\n",
    "        ax.plot(Q[0], Q[1], 'ro', label='Q', markersize=8)\n",
    "        ax.plot(E2[0], E2[1], 'ro', label='E2', markersize=8)\n",
    "        ax.plot(P[0], P[1], 'bo', label='P', markersize=8)\n",
    "        # Draw lines between points\n",
    "        ax.plot([E1[0], P[0]], [E1[1], P[1]], 'b-', lw=2)  # Line between P and E1 (blue)\n",
    "        ax.plot([Q[0], E2[0]], [Q[1], E2[1]], 'r-', lw=2)  # Line between Q and E2 (red)\n",
    "        # Plot intersection point E3 if it exists within the plot limits and above the lines\n",
    "        if E3 is not None and (0 <= E3[0] <= 1.1 * max_N1) and (0 <= E3[1] <= 1.1 * max_N2):\n",
    "            ax.plot(E3[0], E3[1], 'go', label=r'$E_3$', markersize=8)\n",
    "            # Annotate E3 near the point\n",
    "            ax.annotate(f'$E_3$', xy=(E3[0], E3[1]), xytext=(E3[0] + 0.3, E3[1] + 0.3), fontsize=18, color='green')\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel(r'$N_1$', fontsize=18)\n",
    "        ax.set_ylabel(r'$N_2$', fontsize=18)\n",
    "        # Move title to the left\n",
    "        ax.set_title(title, fontsize=18, loc='left')\n",
    "        # Set xticks and yticks with labels for E1, E2, P, Q\n",
    "        ax.set_xticks([0, E1[0], Q[0]])\n",
    "        ax.set_xticklabels([r'$E_0$', r'$E_1$', r'$Q$'])\n",
    "        ax.set_yticks([0, E2[1], P[1]])\n",
    "        ax.set_yticklabels([r'$E_0$', r'$E_2$', r'$P$'])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    # Adjust layout and save the figure\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('img', exist_ok=True)\n",
    "    plt.savefig('img/phase_plane.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEqDensity(focal_species, N_focal, r1, r2, a11, a12, a21, a22):\n",
    "    if focal_species == 0:\n",
    "        return max(0,(r2-1 - a21*N_focal)/a22) if a22!=0 else 0\n",
    "    else:\n",
    "        return max(0,(r1-1 - a12*N_focal)/a11) if a11!=0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNFD(r1, r2, a11, a12, a21, a22, lowN, deltaN):\n",
    "    # Species 1 at low density\n",
    "    N1_low = lowN\n",
    "    N2_low = getEqDensity(0, N1_low, r1, r2, a11, a12, a21, a22)\n",
    "    total_low = N1_low + N2_low\n",
    "    pgr1_low, pgr2_low = getPCG(r1, r2, a11, a12, a21, a22, N1_low, N2_low)\n",
    "    freq1_low = N1_low / total_low if total_low > 0 else 0\n",
    "    # Species 2 at low density\n",
    "    N2_low2 = lowN\n",
    "    N1_low2 = getEqDensity(1, N2_low2, r1, r2, a11, a12, a21, a22)\n",
    "    total_low2 = N1_low2 + N2_low2\n",
    "    pgr1_low2, pgr2_low2 = getPCG(r1, r2, a11, a12, a21, a22, N1_low2, N2_low2)\n",
    "    freq2_low = N2_low2 / total_low2 if total_low2 > 0 else 0\n",
    "    # Species 1 at high density\n",
    "    N1_high = lowN + deltaN\n",
    "    N2_high = getEqDensity(0, N1_high, r1, r2, a11, a12, a21, a22)\n",
    "    total_high = N1_high + N2_high\n",
    "    pgr1_high, pgr2_high = getPCG(r1, r2, a11, a12, a21, a22, N1_high, N2_high)\n",
    "    freq1_high = N1_high / total_high if total_high > 0 else 0\n",
    "    # Species 2 at high density\n",
    "    N2_high2 = lowN + deltaN\n",
    "    N1_high2 = getEqDensity(1, N2_high2, r1, r2, a11, a12, a21, a22)\n",
    "    total_high2 = N1_high2 + N2_high2\n",
    "    pgr1_high2, pgr2_high2 = getPCG(r1, r2, a11, a12, a21, a22, N1_high2, N2_high2)\n",
    "    freq2_high = N2_high2 / total_high2 if total_high2 > 0 else 0\n",
    "    return {\n",
    "        'pgr1': [pgr1_low, pgr1_high],\n",
    "        'freq1': [freq1_low, freq1_high],\n",
    "        'pgr2': [pgr2_low2, pgr2_high2],\n",
    "        'freq2': [freq2_low, freq2_high]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pgr_figures(filter_option, save_fig=False, extinc_crit_1=False):\n",
    "    # Configure global plotting parameters\n",
    "    plt.rcParams.update({\n",
    "        'axes.titlesize': 14,\n",
    "        'axes.labelsize': 18,\n",
    "        'xtick.labelsize': 16,\n",
    "        'ytick.labelsize': 16,\n",
    "        'legend.fontsize': 12,\n",
    "        'font.size': 18,\n",
    "        'lines.linewidth': 1.5\n",
    "    })\n",
    "    summary_path = f\"csv/pgr_analysis_summary_{filter_option}.csv\"\n",
    "    cols = [\n",
    "        'r1', 'r2', 'a11', 'a12', 'a21', 'a22',\n",
    "        'cor_sos', 'nu_sign', 'coexist',\n",
    "        'left_PGR1_dominant', 'right_PGR1_dominant', 'curve_cross'\n",
    "    ]\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(','.join(cols) + '\\n')\n",
    "    lowN = 0.001\n",
    "    deltaN = 100.0\n",
    "    df = pd.read_csv(f\"csv/annplant_2spp_det_rare_filtered_{filter_option}.csv\")\n",
    "    df['cor_sos'] = pd.to_numeric(df['cor_sos'], errors='coerce')\n",
    "    df['Coexist'] = pd.to_numeric(df['Coexist'], errors='coerce').fillna(0).astype(int)\n",
    "    conditions = [\n",
    "        df['cor_sos'] < -0.001,\n",
    "        (df['cor_sos'] >= -0.001) & (df['cor_sos'] <= 0.001),\n",
    "        df['cor_sos'] > 0.001\n",
    "    ]\n",
    "    choices = ['negative', 'zero', 'positive']\n",
    "    df['nu_sign'] = np.select(conditions, choices, default='invalid')\n",
    "    for _, row in df.iterrows():\n",
    "        if row['nu_sign'] == 'invalid':\n",
    "            continue\n",
    "        r1, r2 = row['r1'], row['r2']\n",
    "        a11, a12, a21, a22 = row['a11'], row['a12'], row['a21'], row['a22']\n",
    "        # Compute low- and high-density PGR and frequencies via getNFD\n",
    "        nfd = getNFD(r1, r2, a11, a12, a21, a22, lowN, deltaN)\n",
    "        freq1, pgr1 = nfd['freq1'], nfd['pgr1']\n",
    "        freq2, pgr2 = nfd['freq2'], nfd['pgr2']\n",
    "        # Determine dominance at edges\n",
    "        left_PGR1_dominant = 1 if pgr1[0] > pgr2[0] else 0 if pgr1[0] < pgr2[0] else None\n",
    "        right_PGR1_dominant = 1 if pgr1[1] > pgr2[1] else 0 if pgr1[1] < pgr2[1] else None\n",
    "        curve_cross = 1 if left_PGR1_dominant != right_PGR1_dominant else 0\n",
    "        # Append to summary CSV\n",
    "        line = (\n",
    "            f\"{r1},{r2},{a11},{a12},{a21},{a22},\"\n",
    "            f\"{row['cor_sos']},{row['nu_sign']},{row['Coexist']},\"\n",
    "            f\"{left_PGR1_dominant},{right_PGR1_dominant},{curve_cross}\\n\"\n",
    "        )\n",
    "        with open(summary_path, 'a') as f:\n",
    "            f.write(line)\n",
    "        if save_fig:\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            # Plot species 1 curve: frequency vs log(PGR)\n",
    "            ax.plot(freq1, pgr1, '-', label='N1')\n",
    "            # Plot species 2 curve: frequency vs log(PGR)\n",
    "            ax.plot(freq2, pgr2, '--', label='N2')\n",
    "            ax.axhline(0, color='black', linestyle=':', linewidth=0.8)\n",
    "            ax.set_xlabel('Frequency', fontsize=16)\n",
    "            ax.set_ylabel('log(PGR)', fontsize=16)\n",
    "            ax.set_xlim(-0.005, 1.01)\n",
    "            ax.set_ylim(-1.01, 1.01)\n",
    "            ax.set_title(\n",
    "                f\"\\u03BD={row['cor_sos']:.2g}, Coexist={row['Coexist']}\\n\"\n",
    "                f\"r1={r1:.2g}, a11={a11:.2g}, a12={a12:.2g}\\n\"\n",
    "                f\"r2={r2:.2g}, a21={a21:.2g}, a22={a22:.2g}\",\n",
    "                wrap=True, fontsize=14\n",
    "            )\n",
    "            ax.legend()\n",
    "            top = (\n",
    "                \"PGR_same\" if left_PGR1_dominant is None\n",
    "                else \"PGR1_dominant\" if left_PGR1_dominant\n",
    "                else \"PGR1_nondominant\"\n",
    "            )\n",
    "            nu_folder = {\n",
    "                'positive': \"nu_larger_zero\",\n",
    "                'negative': \"nu_smaller_zero\",\n",
    "                'zero':     \"nu_zero\"\n",
    "            }.get(row['nu_sign'], \"nu_invalid\")\n",
    "            if row['nu_sign'] in ['positive', 'negative', 'zero']:\n",
    "                outcome = \"coexist\" if row['Coexist'] else \"exclusion\"\n",
    "                save_dir = os.path.join(\"png\", top, nu_folder, outcome)\n",
    "            else:\n",
    "                save_dir = os.path.join(\"png\", top, nu_folder)\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            fname = (f\"r1_{r1}_r2_{r2}_a11_{a11}_a12_{a12}_a21_{a21}_a22_{a22}.png\")\n",
    "            fig.savefig(os.path.join(save_dir, fname), dpi=150, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hypotheses_rf(filter_option, summary_path, seed=1234):\n",
    "    df = pd.read_csv(summary_path)\n",
    "    nu_map = {'negative': 0, 'zero': 1, 'positive': 2}\n",
    "    df['nu_sign'] = df['nu_sign'].map(nu_map)\n",
    "    required_features = ['nu_sign', 'curve_cross', 'left_PGR1_dominant'] # Hypotheses, , 'right_PGR1_dominant'\n",
    "    X = df[required_features].copy()\n",
    "    y = df['coexist'].values\n",
    "    unique_classes = np.unique(y)\n",
    "    if unique_classes.size != 2:\n",
    "        raise ValueError(f\"Coexist column must be binary (0/1). Found: {unique_classes}\")\n",
    "    model = RandomForestClassifier(n_estimators=1000, random_state=seed, n_jobs=-1, class_weight='balanced_subsample') # Fit Random Forest\n",
    "    model.fit(X, y)\n",
    "    perm_imp = permutation_importance(model, X, y, n_repeats=100, random_state=seed, n_jobs=-1) # Permutation importance\n",
    "    # SHAP analysis\n",
    "    shap_imp = pd.Series(np.nan, index=X.columns)\n",
    "    shap_vals = None\n",
    "    try:\n",
    "        explainer = shap.Explainer(model, X, feature_perturbation=\"interventional\")\n",
    "        sv = explainer(X)\n",
    "        vals = sv.values\n",
    "        if vals.ndim == 3:\n",
    "            shap_vals = vals[:, :, 1]\n",
    "        elif vals.ndim == 2:\n",
    "            shap_vals = vals\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported SHAP values ndim: {vals.ndim}\")\n",
    "        if shap_vals.shape == (X.shape[1], X.shape[0]):\n",
    "            shap_vals = shap_vals.T\n",
    "        if shap_vals.shape != (X.shape[0], X.shape[1]):\n",
    "            raise ValueError(f\"SHAP/Feature dimension mismatch after transpose check: {shap_vals.shape} vs {X.shape}\")\n",
    "        abs_imp = np.abs(shap_vals).mean(axis=0)\n",
    "        total = abs_imp.sum()\n",
    "        if total > 0:\n",
    "            shap_imp = pd.Series((abs_imp / total).round(3), index=X.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP Calculation Error: {e}\")\n",
    "    # Build and print the importance table\n",
    "    imp_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Permutation Importance': perm_imp.importances_mean.round(3),\n",
    "        'SHAP Impact (%)': (shap_imp * 100).round(1)\n",
    "    }).sort_values('SHAP Impact (%)', ascending=False)\n",
    "    print(\"\\n=== Feature Importance ===\")\n",
    "    print(imp_df.to_string(index=False, float_format=\"%.2g\"))\n",
    "    if shap_vals is not None:\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.summary_plot(\n",
    "                shap_vals, X.values,\n",
    "                feature_names=X.columns,\n",
    "                plot_type='dot', show=False\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "            os.makedirs('shap', exist_ok=True)\n",
    "            plt.savefig(f'shap/shap_{filter_option}.png', dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP Visualization Error: {e}\")\n",
    "    else:\n",
    "        print(\"Skipping SHAP visualization due to missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hypotheses(filter_option, summary_path):\n",
    "    print(\"\\n\" + \"=\"*140)\n",
    "    print(\"Hypothesis Definitions:\")\n",
    "    print(\"\\n(H1) Sign of \\u03BD (cor_sos)\")\n",
    "    print(\"\\n(H2) Curve crossing (left/right edge dominance differs)\")\n",
    "    print(\"\\n(H3) PGR1 at left edge is higher\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    df = pd.read_csv(summary_path)\n",
    "    df['curve_cross'] = (df['left_PGR1_dominant'] != df['right_PGR1_dominant']).astype(int)\n",
    "    print(\"\\n--- Hypothesis Analysis Results ---\")\n",
    "    analyze_hypotheses_rf(filter_option, summary_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_coexistence_deterministic(filter_option):\n",
    "    summary_path = f\"csv/pgr_analysis_summary_{filter_option}.csv\"\n",
    "    if not os.path.exists(summary_path):\n",
    "        print(f\"\\n-----------\\nGenerating analysis data for {filter_option}...\")\n",
    "        plot_pgr_figures(filter_option, save_fig=False, extinc_crit_1=False)\n",
    "    try:\n",
    "        df = pd.read_csv(summary_path)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Empty summary file - regenerate manually\")\n",
    "        df = df[df['nu_sign'].isin(['negative', 'zero', 'positive'])]\n",
    "        df['curve_cross'] = (df['left_PGR1_dominant'] != df['right_PGR1_dominant']).astype(int)\n",
    "        analyze_hypotheses(filter_option, summary_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis failed: {str(e)}\")\n",
    "        return\n",
    "    nu_order = ['negative', 'zero', 'positive']\n",
    "    cross_labels = ['No Cross', 'Cross']\n",
    "    nu_symbols = {'negative': '\\u03BD<0', 'zero': '\\u03BD\\u2248 0', 'positive': '\\u03BD>0'}\n",
    "    plt.rcParams.update({\n",
    "        'axes.titlesize': 14,\n",
    "        'axes.labelsize': 12,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 10,\n",
    "        'font.size': 12,\n",
    "        'lines.linewidth': 1.5\n",
    "    })\n",
    "    # 1. Coexistence vs Curve Crossing\n",
    "    s1 = df.groupby('curve_cross')['coexist'].agg(['sum', 'count'])\n",
    "    s1['p_co'] = s1['sum'] / s1['count']\n",
    "    s1['p_no'] = 1 - s1['p_co']\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    x = np.arange(len(s1))\n",
    "    ax.bar(x, s1['p_co'], color='blue', label='Coexist')\n",
    "    ax.bar(x, s1['p_no'], bottom=s1['p_co'], color='red', label='Non-coexist')\n",
    "    # Annotate each segment\n",
    "    for i in x:\n",
    "        total = s1.iloc[i]['count']\n",
    "        p_co = s1.iloc[i]['p_co']\n",
    "        p_no = s1.iloc[i]['p_no']\n",
    "        sum_co = int(s1.iloc[i]['sum'])\n",
    "        sum_no = int(total - sum_co)\n",
    "        # Coexist segment\n",
    "        ax.text(i, p_co/2, f\"{p_co:.1%}\\n({sum_co})\",  # :.0g\n",
    "                ha='center', va='center', color='white', fontsize=8)\n",
    "        # Non-coexist segment\n",
    "        ax.text(i, p_co + p_no/2, f\"{p_no:.1%}\\n({sum_no})\",  # :.0g\n",
    "                ha='center', va='center', color='white', fontsize=8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cross_labels)\n",
    "    ax.set_ylabel(\"Percentage\")\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda v, _: f\"{v:.0%}\"))\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('img', exist_ok=True)\n",
    "    plt.savefig(f'img/hypothesis_1_{filter_option}.png', dpi=300)\n",
    "    plt.close()\n",
    "    # 2. Coexistence vs nu sign\n",
    "    s2 = df.groupby('nu_sign')['coexist'].agg(['sum', 'count']).reindex(nu_order)\n",
    "    s2['p_co'] = s2['sum'] / s2['count']\n",
    "    s2['p_no'] = 1 - s2['p_co']\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    x = np.arange(len(s2))\n",
    "    ax.bar(x, s2['p_co'], color='blue', label='Coexist')\n",
    "    ax.bar(x, s2['p_no'], bottom=s2['p_co'], color='red', label='Non-coexist')\n",
    "    # Annotate each segment\n",
    "    for i in x:\n",
    "        total = s2.iloc[i]['count']\n",
    "        p_co = s2.iloc[i]['p_co']\n",
    "        p_no = s2.iloc[i]['p_no']\n",
    "        sum_co = int(s2.iloc[i]['sum'])\n",
    "        sum_no = int(total - sum_co)\n",
    "        ax.text(i, p_co/2, f\"{p_co:.1%}\\n({sum_co})\",  # :.0g\n",
    "                ha='center', va='center', color='white', fontsize=8)\n",
    "        ax.text(i, p_co + p_no/2, f\"{p_no:.1%}\\n({sum_no})\",  # :.0g\n",
    "                ha='center', va='center', color='white', fontsize=8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([nu_symbols[nu] for nu in nu_order])\n",
    "    ax.set_ylabel(\"Percentage\")\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda v, _: f\"{v:.0%}\"))\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'img/hypothesis_2_{filter_option}.png', dpi=300)\n",
    "    plt.close()\n",
    "    # 3. Coexistence vs Curve Crossing by nu sign\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "    for ax, nu in zip(axes, nu_order):\n",
    "        sub = df[df['nu_sign'] == nu]\n",
    "        s3 = sub.groupby('curve_cross')['coexist'].agg(['sum', 'count'])\n",
    "        if s3.empty:\n",
    "            continue\n",
    "        s3['p_co'] = s3['sum'] / s3['count']\n",
    "        s3['p_no'] = 1 - s3['p_co']\n",
    "        x_sub = np.arange(len(s3))\n",
    "        ax.bar(x_sub, s3['p_co'], color='blue')\n",
    "        ax.bar(x_sub, s3['p_no'], bottom=s3['p_co'], color='red')\n",
    "        # Annotate each segment\n",
    "        for j in x_sub:\n",
    "            total = s3.iloc[j]['count']\n",
    "            p_co = s3.iloc[j]['p_co']\n",
    "            p_no = s3.iloc[j]['p_no']\n",
    "            sum_co = int(s3.iloc[j]['sum'])\n",
    "            sum_no = int(total - sum_co)\n",
    "            ax.text(j, p_co/2, f\"{p_co:.1%}\\n({sum_co})\",  # :.0g\n",
    "                    ha='center', va='center', color='white', fontsize=8)\n",
    "            ax.text(j, p_co + p_no/2, f\"{p_no:.1%}\\n({sum_no})\",  # :.0g\n",
    "                    ha='center', va='center', color='white', fontsize=8)\n",
    "        ax.set_xticks(x_sub)\n",
    "        ax.set_xticklabels(cross_labels)\n",
    "        ax.set_title(nu_symbols[nu])\n",
    "        if ax == axes[0]:\n",
    "            ax.set_ylabel(\"Percentage\")\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=c, edgecolor='k') for c in ['blue', 'red']]\n",
    "    fig.legend(handles, ['Coexist', 'Non-coexist'], loc='upper right', bbox_to_anchor=(0.99, 0.99))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'img/hypothesis_3_{filter_option}.png', dpi=300)\n",
    "    plt.close()\n",
    "    # 4. Heatmap: Coexistence count/total and percentage\n",
    "    heat = df.groupby(['nu_sign', 'curve_cross']).agg(\n",
    "        total=('coexist', 'count'),\n",
    "        coexist=('coexist', 'sum')\n",
    "    ).reset_index().set_index(['nu_sign', 'curve_cross'])\n",
    "    # Prepare matrices\n",
    "    total_mat = heat['total'].unstack().reindex(index=nu_order, columns=[0, 1]).fillna(0)\n",
    "    co_mat = heat['coexist'].unstack().reindex(index=nu_order, columns=[0, 1]).fillna(0)\n",
    "    pct_mat = co_mat / total_mat.replace(0, np.nan)\n",
    "    # Create annotation text: coexist/total (pct%)\n",
    "    annot = (co_mat.astype(int).astype(str) + \"/\" + total_mat.astype(int).astype(str) + \n",
    "             \"\\n(\" + (pct_mat * 100).round(1).astype(str) + \"%)\").values\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        pct_mat,\n",
    "        annot=annot,\n",
    "        fmt='',\n",
    "        cmap=\"YlGnBu\",\n",
    "        cbar_kws={'label': 'Coexistence %'},\n",
    "        linewidths=0.5,\n",
    "        linecolor='grey',\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel(\"Curve Crossing\")\n",
    "    ax.set_ylabel(\"\\u03BD Sign\")\n",
    "    ax.set_xticklabels(cross_labels)\n",
    "    ax.set_yticklabels([nu_symbols[nu] for nu in nu_order], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'img/coexistence_heatmap_{filter_option}.png', dpi=300)\n",
    "    plt.close()\n",
    "    # 5. Core Analysis Tables\n",
    "    analysis_data = df.groupby(\n",
    "        ['nu_sign', 'curve_cross', 'left_PGR1_dominant', 'coexist']\n",
    "    ).size().unstack(fill_value=0)\n",
    "    analysis_data['Total'] = analysis_data.sum(axis=1)\n",
    "    # Ensure consistent ordering\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [nu_order, [0, 1], [0, 1]],\n",
    "        names=['\\u03BD Sign', 'Curve Cross', 'Left N1 Dominant']\n",
    "    )\n",
    "    analysis_data = analysis_data.reindex(index, fill_value=0)\n",
    "    total_all = analysis_data['Total'].sum()\n",
    "    formatted = analysis_data.copy()\n",
    "    for col in [0, 1]:\n",
    "        formatted[col] = (\n",
    "            analysis_data[col] / total_all\n",
    "        ).map(\"{:.1%}\".format) + \" (\" + analysis_data[col].astype(int).astype(str) + \")\"\n",
    "    formatted.to_csv(f'csv/coexistence_output_{filter_option}.csv')\n",
    "    print(\"\\n=== Coexistence Analysis ===\")\n",
    "    print(formatted.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_pipeline(filters, base_file, solver, truncate, save_fig, extinc_crit_1):\n",
    "    os.makedirs('csv', exist_ok=True)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    if not os.path.exists(base_file):\n",
    "        print(\"Running simulation...\")\n",
    "        mesh = preprocess_data('table1')\n",
    "        results = [Sim(k, row, extinc_crit_1=extinc_crit_1, solver=solver) \n",
    "                   for k, row in tqdm(enumerate(mesh), total=len(mesh))]\n",
    "        postprocess_results(results, base_file)\n",
    "    for filter_option in filters:\n",
    "        filtered_filename = f\"csv/annplant_2spp_det_rare_filtered_{filter_option}.csv\"\n",
    "        if not os.path.exists(filtered_filename):\n",
    "            print(f\"\\nGenerating data for filter={filter_option}...\")\n",
    "            cor_figure(filter_option, truncate)\n",
    "        summary_path = f\"csv/pgr_analysis_summary_{filter_option}.csv\"\n",
    "        if not os.path.exists(summary_path):\n",
    "            plot_pgr_figures(filter_option, save_fig, extinc_crit_1=False)\n",
    "        try:\n",
    "            filtered_data = pd.read_csv(filtered_filename)\n",
    "            bool_cols = ['Coexist', 'A', 'B', 'C', 'D']\n",
    "            for col in bool_cols:\n",
    "                if col in filtered_data.columns:\n",
    "                    filtered_data[col] = filtered_data[col].astype(bool)\n",
    "            print(\"\\nAnalysis:\")\n",
    "            analyze_coexistence_effect(filtered_data, False)\n",
    "            plot_phase_plane()\n",
    "            count_abcd(filtered_data)\n",
    "            if 'C' in filtered_data.columns:\n",
    "                compare_counts_test(filtered_data[filtered_data['C']], True)\n",
    "        except Exception as e:\n",
    "            print(f\"Processing error: {str(e)}\")\n",
    "        analyze_coexistence_deterministic(filter_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filters = ['on', 'off']\n",
    "    base_file = \"csv/annplant_2spp_det_rare.csv\"\n",
    "    solver = 'dynamics' # 'analyN'\n",
    "    truncate = False\n",
    "    save_fig = True\n",
    "    extinc_crit_1 = False\n",
    "    setup_pipeline(filters, base_file, solver, truncate, save_fig, extinc_crit_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simulation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 77760/77760 [02:25<00:00, 535.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating data for filter=on...\n",
      "\n",
      "Analysis:\n",
      "\n",
      "--- Analysis for SoS ---\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                Coexist   No. Observations:                17823\n",
      "Model:                            GLM   Df Residuals:                    17819\n",
      "Model Family:                Binomial   Df Model:                            3\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -898.85\n",
      "Date:                Thu, 12 Jun 2025   Deviance:                       1797.7\n",
      "Time:                        17:40:32   Pearson chi2:                 3.69e+03\n",
      "No. Iterations:                    32   Pseudo R-squ. (CS):            0.06074\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       -242.4498   1.36e+05     -0.002      0.999   -2.67e+05    2.66e+05\n",
      "S1           229.6854   1.36e+05      0.002      0.999   -2.66e+05    2.67e+05\n",
      "FE1           18.3541      0.957     19.173      0.000      16.478      20.230\n",
      "cor_sos       -0.0019      0.000     -4.711      0.000      -0.003      -0.001\n",
      "==============================================================================\n",
      "\n",
      "Analysis on Negative  for COR_SOS:\n",
      "Proportion of coexistence with   0: 0.98 (95% CI: (0.9807090670798713, 0.9846796905688473))\n",
      "Proportion of coexistence with  < 0: 1 (95% CI: (0.9970649930174468, 1.0000000000000002))\n",
      "Coexistence and Exclusion based on  for SoS:\n",
      "                0   < 0\n",
      "Coexistence  16234   1305\n",
      "Exclusion      284      0\n",
      "Higher coexistence observed with  < 0 for SoS, supporting the authors' results.\n",
      "A\n",
      "Coexist==0: 0\n",
      "Coexist==1: 0\n",
      "Total: 0\n",
      "Proportion: 0\n",
      "\n",
      "B\n",
      "Coexist==0: 284\n",
      "Coexist==1: 5710\n",
      "Total: 5994\n",
      "Proportion: 0.34\n",
      "\n",
      "C\n",
      "Coexist==0: 0\n",
      "Coexist==1: 11829\n",
      "Total: 11829\n",
      "Proportion: 0.66\n",
      "\n",
      "D\n",
      "Coexist==0: 0\n",
      "Coexist==1: 0\n",
      "Total: 0\n",
      "Proportion: 0\n",
      "\n",
      "PGR Statistics:\n",
      "\n",
      "PGR1 Mean  SD: -3.3e-07  6.2e-07\n",
      "PGR2 Mean  SD: 2e-08  1.5e-08\n",
      "\n",
      "============================================================================================================================================\n",
      "Hypothesis Definitions:\n",
      "\n",
      "(H1) Sign of  (cor_sos)\n",
      "\n",
      "(H2) Curve crossing (left/right edge dominance differs)\n",
      "\n",
      "(H3) PGR1 at left edge is higher\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Hypothesis Analysis Results ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 35607/35646 [02:22<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Importance ===\n",
      "           Feature  Permutation Importance  SHAP Impact (%)\n",
      "       curve_cross                  -0.039               63\n",
      "           nu_sign                  -0.031               30\n",
      "left_PGR1_dominant                  -0.039              7.2\n",
      "\n",
      "=== Coexistence Analysis ===\n",
      "coexist                                         0              1  Total\n",
      " Sign   Curve Cross Left N1 Dominant                                  \n",
      "negative 0           0                   0.0% (0)     3.1% (555)    555\n",
      "                     1                   0.0% (0)       0.0% (0)      0\n",
      "         1           0                   0.0% (0)      0.3% (57)     57\n",
      "                     1                   0.0% (0)     3.9% (693)    693\n",
      "zero     0           0                  0.3% (46)     1.7% (304)    350\n",
      "                     1                   0.0% (0)       0.0% (0)      0\n",
      "         1           0                   0.0% (0)     1.2% (214)    214\n",
      "                     1                   0.0% (0)       0.0% (0)      0\n",
      "positive 0           0                 1.3% (238)  82.0% (14617)  14855\n",
      "                     1                   0.0% (0)       0.0% (0)      0\n",
      "         1           0                   0.0% (0)     5.2% (922)    922\n",
      "                     1                   0.0% (0)     1.0% (177)    177\n",
      "\n",
      "Generating data for filter=off...\n",
      "\n",
      "Analysis:\n",
      "\n",
      "--- Analysis for SoS ---\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                Coexist   No. Observations:                46015\n",
      "Model:                            GLM   Df Residuals:                    46011\n",
      "Model Family:                Binomial   Df Model:                            3\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -5119.8\n",
      "Date:                Thu, 12 Jun 2025   Deviance:                       10240.\n",
      "Time:                        20:19:10   Pearson chi2:                 3.49e+04\n",
      "No. Iterations:                    10   Pseudo R-squ. (CS):             0.6866\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -25.6301      0.400    -64.140      0.000     -26.413     -24.847\n",
      "S1            19.5236      0.271     72.067      0.000      18.993      20.055\n",
      "FE1           12.3576      0.251     49.278      0.000      11.866      12.849\n",
      "cor_sos        0.0033      0.000     10.090      0.000       0.003       0.004\n",
      "==============================================================================\n",
      "\n",
      "Analysis on Negative  for COR_SOS:\n",
      "Proportion of coexistence with   0: 0.52 (95% CI: (0.5131072512216055, 0.5224772161942033))\n",
      "Proportion of coexistence with  < 0: 0.75 (95% CI: (0.7293343944156329, 0.7646752100875024))\n",
      "Coexistence and Exclusion based on  for SoS:\n",
      "                0   < 0\n",
      "Coexistence  22625   1734\n",
      "Exclusion    21070    586\n",
      "Higher coexistence observed with  < 0 for SoS, supporting the authors' results.\n",
      "A\n",
      "Coexist==0: 0\n",
      "Coexist==1: 0\n",
      "Total: 0\n",
      "Proportion: 0\n",
      "\n",
      "B\n",
      "Coexist==0: 17180\n",
      "Coexist==1: 11449\n",
      "Total: 28629\n",
      "Proportion: 0.62\n",
      "\n",
      "C\n",
      "Coexist==0: 0\n",
      "Coexist==1: 12192\n",
      "Total: 12192\n",
      "Proportion: 0.26\n",
      "\n",
      "D\n",
      "Coexist==0: 4476\n",
      "Coexist==1: 718\n",
      "Total: 5194\n",
      "Proportion: 0.11\n",
      "\n",
      "PGR Statistics:\n",
      "\n",
      "PGR1 Mean  SD: -4e-07  1e-06\n",
      "PGR2 Mean  SD: 2.1e-08  1.5e-08\n",
      "\n",
      "============================================================================================================================================\n",
      "Hypothesis Definitions:\n",
      "\n",
      "(H1) Sign of  (cor_sos)\n",
      "\n",
      "(H2) Curve crossing (left/right edge dominance differs)\n",
      "\n",
      "(H3) PGR1 at left edge is higher\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Hypothesis Analysis Results ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|===================| 91783/92030 [05:53<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Importance ===\n",
      "           Feature  Permutation Importance  SHAP Impact (%)\n",
      "       curve_cross                   0.045               60\n",
      "           nu_sign                   0.017               32\n",
      "left_PGR1_dominant                   0.005              8.2\n",
      "\n",
      "=== Coexistence Analysis ===\n",
      "coexist                                            0              1  Total\n",
      " Sign   Curve Cross Left N1 Dominant                                     \n",
      "negative 0           0                    0.2% (104)     1.4% (664)    768\n",
      "                     1                    0.9% (397)     0.4% (203)    600\n",
      "         1           0                     0.2% (85)     0.4% (174)    259\n",
      "                     1                      0.0% (0)     1.5% (693)    693\n",
      "zero     0           0                     0.1% (46)     0.7% (304)    350\n",
      "                     1                      0.0% (0)       0.0% (0)      0\n",
      "         1           0                      0.0% (6)     0.5% (214)    220\n",
      "                     1                      0.0% (0)       0.0% (0)      0\n",
      "positive 0           0                 43.9% (20215)  43.0% (19784)  39999\n",
      "                     1                     0.2% (73)      0.1% (55)    128\n",
      "         1           0                    1.6% (730)    4.5% (2091)   2821\n",
      "                     1                      0.0% (0)     0.4% (177)    177\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
