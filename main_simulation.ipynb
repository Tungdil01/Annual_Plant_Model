{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code is a modification of the Yenni et al. (2012) analysis:\n",
    "#### - runs the analysis with and without the filter S1 >= 1 & S2 >= 1\n",
    "#### - includes Cushing et al. (2004) analytical results\n",
    "\n",
    "#### their original code: https://github.com/gmyenni/RareStabilizationSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import qmc, norm\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'axes.labelsize': 16,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'font.size': 16,\n",
    "    'axes.grid': False,\n",
    "    'text.usetex': False,\n",
    "    'mathtext.fontset': 'stix',\n",
    "    'font.family': 'STIXGeneral'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_equilibria(params, model, eps=1e-8):\n",
    "    r1, r2, a11, a22, a12, a21 = params\n",
    "    if model == 'bevertonHolt':\n",
    "        det = a11 * a22 - a12 * a21\n",
    "        if abs(det) <= eps:\n",
    "            return {}\n",
    "        N1 = (a22 * (r1 - 1) - a12 * (r2 - 1)) / det\n",
    "        N2 = (a11 * (r2 - 1) - a21 * (r1 - 1)) / det\n",
    "        # Strength of Stabilization Adler et al. (2007)\n",
    "        S1 = r2 / (1.0 + (a12 / a22) * (r2 - 1))\n",
    "        S2 = r1 / (1.0 + (a21 / a11) * (r1 - 1))\n",
    "        # Competition Ability Hart et al. (2018)\n",
    "        CA1 = (r1 - 1) / np.sqrt(a12 * a11) if (a12 > eps and a11 > eps) else np.nan\n",
    "        CA2 = (r2 - 1) / np.sqrt(a21 * a22) if (a21 > eps and a22 > eps) else np.nan\n",
    "        # Competition Effect Streipert and Wolkowicz (2022)\n",
    "        CE1 = (r1 - 1)/a12 - (r2 - 1)/a22 if (a12 > eps and a22 > eps) else np.nan\n",
    "        CE2 = (r2 - 1)/a21 - (r1 - 1)/a11 if (a21 > eps and a11 > eps) else np.nan\n",
    "        # Carrying capacity for Beverton-Holt: (r-1)/a\n",
    "        N1_potential = (r1 - 1) / a11 if (a11 > eps and (r1 - 1) > 0.0) else eps\n",
    "        N2_potential = (r2 - 1) / a22 if (a22 > eps and (r2 - 1) > 0.0) else eps\n",
    "    elif model == 'ricker':\n",
    "        det = a11 * a22 - a12 * a21\n",
    "        if abs(det) <= eps:\n",
    "            return {}\n",
    "        N1 = (r1 * a22 - r2 * a12) / det\n",
    "        N2 = (r2 * a11 - r1 * a21) / det\n",
    "        S1 = np.exp(r2 * (1.0 - a12 / a22))\n",
    "        S2 = np.exp(r1 * (1.0 - a21 / a11))\n",
    "        # Carrying capacity for Ricker: r/a\n",
    "        N1_potential = r1 / a11 if a11 > eps else eps\n",
    "        N2_potential = r2 / a22 if a22 > eps else eps\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model: %s\" % model)\n",
    "    nu_a = (N1 - N2) * (a11 - a22) / 2.0\n",
    "    nu = (N1 - N2) * (S1 - S2) / 2.0 # Strength of Self-limitation Yenni et al. (2012)\n",
    "    N1_realized = N1 if N1 > 0.0 else 0.0\n",
    "    N2_realized = N2 if N2 > 0.0 else 0.0\n",
    "    N1_potential = np.maximum(N1_potential, eps)\n",
    "    N2_potential = np.maximum(N2_potential, eps)\n",
    "    ASL1 = a11 * np.sqrt(N1_realized**2 + N1_potential**2)\n",
    "    ASL2 = a22 * np.sqrt(N2_realized**2 + N2_potential**2)\n",
    "    nu_ASL = (N1 - N2) * (ASL1 - ASL2) / 2.0\n",
    "    if model == 'ricker':\n",
    "        return {\n",
    "            'N1': N1, 'N2': N2, 'S1': S1, 'S2': S2,\n",
    "            'nu': nu, 'nu_ASL': nu_ASL, 'nu_a': nu_a\n",
    "        }\n",
    "    else:\n",
    "        nu_CA = (N1 - N2) * (CA1 - CA2) / 2.0\n",
    "        nu_CE = (N1 - N2) * (CE1 - CE2) / 2.0\n",
    "        return {\n",
    "            'N1': N1, 'N2': N2, 'S1': S1, 'S2': S2,\n",
    "            'nu': nu, 'nu_ASL': nu_ASL, 'nu_a': nu_a,\n",
    "            'nu_CA': nu_CA, 'nu_CE': nu_CE\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_analytical_scenarios_beverton_holt(params):\n",
    "    r1, r2, a11, a22, a12, a21 = params\n",
    "    if r1 <= 1 or r2 <= 1:\n",
    "        return 'invalid' # Avoid division by zero\n",
    "    # Calculate the analytical conditions\n",
    "    cond1_left = a12\n",
    "    cond1_right = a22 * (r1 - 1) / (r2 - 1)\n",
    "    cond2_left = a21\n",
    "    cond2_right = a11 * (r2 - 1) / (r1 - 1)\n",
    "    # Check the four scenarios\n",
    "    if cond1_left < cond1_right and cond2_left > cond2_right:\n",
    "        return 'species1_wins'\n",
    "    elif cond1_left > cond1_right and cond2_left < cond2_right:\n",
    "        return 'species2_wins'\n",
    "    elif cond1_left < cond1_right and cond2_left < cond2_right:\n",
    "        return 'stable_coexistence'\n",
    "    elif cond1_left > cond1_right and cond2_left > cond2_right:\n",
    "        return 'saddle_point'\n",
    "    else:\n",
    "        return 'borderline' # Edge cases where inequalities are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_analytical_scenarios_ricker(params):\n",
    "    r1, r2, a11, a22, a12, a21 = params\n",
    "    if r1 <= 0 or r2 <= 0:\n",
    "        return 'invalid'\n",
    "    # Calculate the analytical conditions\n",
    "    cond1 = a12 < (r1 * a22 / r2)\n",
    "    cond2 = a21 < (r2 * a11 / r1)\n",
    "    # Check the four scenarios\n",
    "    if cond1 and cond2:\n",
    "        return 'stable_coexistence'\n",
    "    elif cond1 and not cond2:\n",
    "        return 'species1_wins'\n",
    "    elif not cond1 and cond2:\n",
    "        return 'species2_wins'\n",
    "    else:\n",
    "        return 'borderline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_percentile_proportion(event_mask, condition_mask=None, replicates=1000, seed=1234, alpha=0.05):\n",
    "    if replicates <= 0:\n",
    "        raise ValueError(\"bootstrap_percentile_proportion: replicates must be > 0\")\n",
    "    rng = np.random.default_rng(seed)\n",
    "    event_mask = np.asarray(event_mask, dtype=bool)\n",
    "    n = len(event_mask)\n",
    "    if condition_mask is None:\n",
    "        condition_mask = np.ones(n, dtype=bool)\n",
    "    condition_mask = np.asarray(condition_mask, dtype=bool)\n",
    "    if len(condition_mask) != n:\n",
    "        raise ValueError(\"bootstrap_percentile_proportion: event_mask and condition_mask must have same length.\")\n",
    "    denom_obs = int(condition_mask.sum())\n",
    "    if denom_obs == 0:\n",
    "        raise ValueError(\"bootstrap_percentile_proportion: conditioning set is empty\")\n",
    "    est = float(np.count_nonzero(event_mask & condition_mask)) / float(denom_obs) # Observed estimate\n",
    "    rep = np.empty(replicates, dtype=float) # Bootstrap replicates\n",
    "    for i in range(replicates):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        denom = int(condition_mask[idx].sum())\n",
    "        if denom == 0: # Skip this replicate if condition empty\n",
    "            rep[i] = np.nan\n",
    "            continue\n",
    "        num = int(np.count_nonzero(event_mask[idx] & condition_mask[idx]))\n",
    "        rep[i] = float(num) / float(denom)\n",
    "    rep = rep[~np.isnan(rep)] # Remove NaN values\n",
    "    if len(rep) == 0:\n",
    "        return est, np.nan, np.nan, np.array([])\n",
    "    lower = float(np.percentile(rep, 100.0 * (alpha / 2.0)))\n",
    "    upper = float(np.percentile(rep, 100.0 * (1.0 - alpha / 2.0)))\n",
    "    return est, lower, upper, rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_parameters_lhs(n_samples=10000, seed=1234):\n",
    "    r_range = (1.01, 20.0)\n",
    "    a_range = (0.01, 3.0)\n",
    "    bounds = np.array([r_range, r_range, a_range, a_range, a_range, a_range])\n",
    "    low = bounds[:, 0]\n",
    "    high = bounds[:, 1]\n",
    "    sampler = qmc.LatinHypercube(d=6, seed=seed)\n",
    "    u = sampler.random(n=n_samples)\n",
    "    params = qmc.scale(u, low, high)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_parameter_set(params, model, metrics, eps = 1e-8):\n",
    "    r1, r2, a11, a22, a12, a21 = params\n",
    "    if model == 'bevertonHolt':\n",
    "        scenario = check_analytical_scenarios_beverton_holt(params)\n",
    "        coexistence = (scenario == 'stable_coexistence')\n",
    "        competitive_exclusion = (scenario == 'species1_wins' or scenario == 'species2_wins')\n",
    "        valid_outcome = (coexistence or competitive_exclusion)\n",
    "    else:\n",
    "        scenario = check_analytical_scenarios_ricker(params)\n",
    "        coexistence = (scenario == 'stable_coexistence')\n",
    "        competitive_exclusion = (scenario == 'species1_wins' or scenario == 'species2_wins')\n",
    "        valid_outcome = (coexistence or competitive_exclusion)\n",
    "    if not valid_outcome:\n",
    "        result = {'valid_outcome': False, 'coexistence': False, 'competitive_exclusion': False, 'has_rare_species': False}\n",
    "        for metric in metrics:\n",
    "            result[metric] = np.nan\n",
    "        return result\n",
    "    if coexistence:\n",
    "        equilibria = compute_equilibria(params, model)\n",
    "        if not equilibria or any(np.isnan(equilibria.get(metric, np.nan)) for metric in metrics):\n",
    "            result = {'valid_outcome': False, 'coexistence': False, 'competitive_exclusion': False, 'has_rare_species': False}\n",
    "            for metric in metrics:\n",
    "                result[metric] = np.nan\n",
    "            return result\n",
    "        N1 = equilibria.get('N1', 0.0)\n",
    "        N2 = equilibria.get('N2', 0.0)\n",
    "    else:\n",
    "        if model == 'bevertonHolt':\n",
    "            if scenario == 'species1_wins':\n",
    "                N1 = (r1 - 1) / a11 if (a11 > eps and (r1 - 1) > 0.0) else 0.0\n",
    "                N2 = 0.0\n",
    "                N1_potential = (r1 - 1) / a11 if (a11 > eps and (r1 - 1) > 0.0) else eps\n",
    "                N2_potential = (r2 - 1) / a22 if (a22 > eps and (r2 - 1) > 0.0) else eps\n",
    "            else:\n",
    "                N1 = 0.0\n",
    "                N2 = (r2 - 1) / a22 if (a22 > eps and (r2 - 1) > 0.0) else 0.0\n",
    "                N1_potential = (r1 - 1) / a11 if (a11 > eps and (r1 - 1) > 0.0) else eps\n",
    "                N2_potential = (r2 - 1) / a22 if (a22 > eps and (r2 - 1) > 0.0) else eps\n",
    "            S1 = r2 / (1.0 + (a12 / a22) * (r2 - 1))\n",
    "            S2 = r1 / (1.0 + (a21 / a11) * (r1 - 1))\n",
    "            CA1 = (r1 - 1) / np.sqrt(a12 * a11) if (a12 > eps and a11 > eps) else np.nan\n",
    "            CA2 = (r2 - 1) / np.sqrt(a21 * a22) if (a21 > eps and a22 > eps) else np.nan\n",
    "            CE1 = (r1 - 1)/a12 - (r2 - 1)/a22 if (a12 > eps and a22 > eps) else np.nan\n",
    "            CE2 = (r2 - 1)/a21 - (r1 - 1)/a11 if (a21 > eps and a11 > eps) else np.nan\n",
    "        else:\n",
    "            if scenario == 'species1_wins':\n",
    "                N1 = r1 / a11 if a11 > eps else 0.0\n",
    "                N2 = 0.0\n",
    "                N1_potential = r1 / a11 if a11 > eps else eps\n",
    "                N2_potential = r2 / a22 if a22 > eps else eps\n",
    "            else:\n",
    "                N1 = 0.0\n",
    "                N2 = r2 / a22 if a22 > eps else 0.0\n",
    "                N1_potential = r1 / a11 if a11 > eps else eps\n",
    "                N2_potential = r2 / a22 if a22 > eps else eps\n",
    "            S1 = np.exp(r2 * (1.0 - a12 / a22))\n",
    "            S2 = np.exp(r1 * (1.0 - a21 / a11))\n",
    "            CA1 = np.nan\n",
    "            CA2 = np.nan\n",
    "            CE1 = np.nan\n",
    "            CE2 = np.nan\n",
    "        nu = (N1 - N2) * (S1 - S2) / 2.0\n",
    "        nu_a = (N1 - N2) * (a11 - a22) / 2.0\n",
    "        N1_realized = N1 if N1 > 0.0 else 0.0\n",
    "        N2_realized = N2 if N2 > 0.0 else 0.0\n",
    "        N1_potential = np.maximum(N1_potential, eps)\n",
    "        N2_potential = np.maximum(N2_potential, eps)\n",
    "        ASL1 = a11 * np.sqrt(N1_realized**2 + N1_potential**2)\n",
    "        ASL2 = a22 * np.sqrt(N2_realized**2 + N2_potential**2)\n",
    "        nu_ASL = (N1 - N2) * (ASL1 - ASL2) / 2.0\n",
    "        if model == 'bevertonHolt':\n",
    "            nu_CA = (N1 - N2) * (CA1 - CA2) / 2.0\n",
    "            nu_CE = (N1 - N2) * (CE1 - CE2) / 2.0\n",
    "            equilibria = {\n",
    "                'N1': N1, 'N2': N2, 'S1': S1, 'S2': S2,\n",
    "                'nu': nu, 'nu_ASL': nu_ASL, 'nu_a': nu_a,\n",
    "                'nu_CA': nu_CA, 'nu_CE': nu_CE\n",
    "            }\n",
    "        else:\n",
    "            equilibria = {\n",
    "                'N1': N1, 'N2': N2, 'S1': S1, 'S2': S2,\n",
    "                'nu': nu, 'nu_ASL': nu_ASL, 'nu_a': nu_a\n",
    "            }\n",
    "    result = {\n",
    "        'valid_outcome': True,\n",
    "        'coexistence': coexistence,\n",
    "        'competitive_exclusion': competitive_exclusion,\n",
    "        'has_rare_species': False\n",
    "    }\n",
    "    for metric in metrics:\n",
    "        result[metric] = equilibria.get(metric, np.nan) if equilibria else np.nan\n",
    "    N1_pos = max(N1, 0.0)\n",
    "    N2_pos = max(N2, 0.0)\n",
    "    total_pop = N1_pos + N2_pos\n",
    "    if total_pop > eps:\n",
    "        frac1 = N1_pos / total_pop\n",
    "        frac2 = N2_pos / total_pop\n",
    "        rarity_threshold = 0.25\n",
    "        result['has_rare_species'] = (frac1 < rarity_threshold) or (frac2 < rarity_threshold) or (N1_pos <= eps) or (N2_pos <= eps)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(y_true, y_pred):\n",
    "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_comparison_metrics(results, params, metrics, scenario, bootstrap_replicates=1000):\n",
    "    valid_mask = results['valid_outcome']\n",
    "    if not np.any(valid_mask):\n",
    "        print(\"Warning: No valid outcomes for %s scenario\" % scenario)\n",
    "        return None, None, None, None\n",
    "    if scenario == 'general':\n",
    "        mask = valid_mask\n",
    "    elif scenario == 'rarity':\n",
    "        rare_mask = results['has_rare_species'][valid_mask]\n",
    "        if np.sum(rare_mask) == 0:\n",
    "            print(\"Warning: No rare species cases for %s scenario\" % scenario)\n",
    "            return None, None, None, None\n",
    "        mask = valid_mask.copy()\n",
    "        mask[valid_mask] = rare_mask\n",
    "    else:\n",
    "        raise ValueError(\"Unknown scenario: %s\" % scenario)\n",
    "    metrics_data = {}\n",
    "    for metric in metrics:\n",
    "        metrics_data[metric] = results[metric][mask]\n",
    "    outcomes_raw = results['coexistence'][mask].astype(int)\n",
    "    clean_mask = np.ones(len(outcomes_raw), dtype=bool)\n",
    "    for metric in metrics:\n",
    "        clean_mask = clean_mask & (~np.isnan(metrics_data[metric]))\n",
    "    if not np.any(clean_mask):\n",
    "        print(\"Warning: No non-NaN rows after cleaning for %s scenario\" % scenario)\n",
    "        return None, None, None, None\n",
    "    for metric in metrics:\n",
    "        metrics_data[metric] = np.asarray(metrics_data[metric])[clean_mask]\n",
    "    outcomes = np.asarray(outcomes_raw)[clean_mask].astype(int)\n",
    "    if len(outcomes) == 0:\n",
    "        print(\"Warning: No valid data for %s scenario after NaN cleaning\" % scenario)\n",
    "        return None, None, None, None\n",
    "    optimal_predictions = {}\n",
    "    optimal_thresholds = {}\n",
    "    optimal_directions = {}\n",
    "    optimal_mcc_values = {}\n",
    "    optimal_accuracies = {}\n",
    "    auc_values = {}\n",
    "    for metric in metrics:\n",
    "        metric_vals = metrics_data[metric]\n",
    "        if len(np.unique(outcomes)) < 2:\n",
    "            optimal_predictions[metric] = np.zeros_like(outcomes)\n",
    "            optimal_thresholds[metric] = 0.0\n",
    "            optimal_directions[metric] = 'below'\n",
    "            optimal_mcc_values[metric] = 0.0\n",
    "            optimal_accuracies[metric] = 0.0\n",
    "            auc_values[metric] = 0.5\n",
    "            continue\n",
    "        sorted_vals = np.sort(metric_vals)\n",
    "        percentiles = np.linspace(1, 99, 99)\n",
    "        thresholds = np.percentile(sorted_vals, percentiles)\n",
    "        best_mcc = -1\n",
    "        best_threshold = 0.0\n",
    "        best_direction = 'below'\n",
    "        best_pred = None\n",
    "        for threshold in thresholds:\n",
    "            for direction in ['below', 'above']:\n",
    "                if direction == 'below':\n",
    "                    predictions = (metric_vals < threshold).astype(int)\n",
    "                else:\n",
    "                    predictions = (metric_vals > threshold).astype(int)\n",
    "                if len(np.unique(predictions)) < 2:\n",
    "                    continue\n",
    "                try:\n",
    "                    mcc = matthews_corrcoef(outcomes, predictions)\n",
    "                except:\n",
    "                    mcc = -1\n",
    "                if mcc > best_mcc:\n",
    "                    best_mcc = mcc\n",
    "                    best_threshold = threshold\n",
    "                    best_direction = direction\n",
    "                    best_pred = predictions\n",
    "        if best_pred is None:\n",
    "            best_pred = (metric_vals < 0).astype(int)\n",
    "            best_threshold = 0.0\n",
    "            best_direction = 'below'\n",
    "            best_mcc = matthews_corrcoef(outcomes, best_pred) if len(np.unique(best_pred)) >= 2 else 0.0\n",
    "        optimal_predictions[metric] = best_pred\n",
    "        optimal_thresholds[metric] = best_threshold\n",
    "        optimal_directions[metric] = best_direction\n",
    "        optimal_mcc_values[metric] = best_mcc\n",
    "        optimal_accuracies[metric] = float(np.sum(best_pred == outcomes)) / float(len(outcomes)) if len(outcomes) > 0 else 0.0\n",
    "        if len(np.unique(metric_vals)) > 1:\n",
    "            fpr, tpr, _ = roc_curve(outcomes, metric_vals)\n",
    "            auc_val = auc(fpr, tpr)\n",
    "            if auc_val < 0.5:\n",
    "                auc_val = 1 - auc_val\n",
    "            auc_values[metric] = auc_val\n",
    "        else:\n",
    "            auc_values[metric] = 0.5\n",
    "    candidate_items = [(n, auc_values[n]) for n in metrics if n in auc_values]\n",
    "    if len(candidate_items) == 0:\n",
    "        best_name = None\n",
    "        best_score = 0.0\n",
    "        worse_score = 0.0\n",
    "    else:\n",
    "        best_name = max(candidate_items, key=lambda x: x[1])[0]\n",
    "        best_score = auc_values.get(best_name, 0.0)\n",
    "        sorted_scores = sorted([v for n, v in candidate_items], reverse=True)\n",
    "        worse_score = sorted_scores[1] if len(sorted_scores) > 1 else 0.0\n",
    "    metrics_dict = {\n",
    "        'scenario': scenario,\n",
    "        'n_cases': len(outcomes),\n",
    "        'n_cases_raw': int(np.sum(mask)),\n",
    "        'coexistence_count': int(np.sum(outcomes == 1)),\n",
    "        'competitive_exclusion_count': int(np.sum(outcomes == 0)),\n",
    "        'better_metric': best_name,\n",
    "        'better_score': best_score,\n",
    "        'worse_score': worse_score\n",
    "    }\n",
    "    for metric in metrics:\n",
    "        metrics_dict[f'{metric}_auc'] = auc_values.get(metric, 0.5)\n",
    "        metrics_dict[f'{metric}_mcc'] = optimal_mcc_values.get(metric, 0.0)\n",
    "        metrics_dict[f'{metric}_accuracy'] = optimal_accuracies.get(metric, 0.0)\n",
    "        metrics_dict[f'{metric}_threshold'] = optimal_thresholds.get(metric, 0.0)\n",
    "        metrics_dict[f'{metric}_direction'] = optimal_directions.get(metric, 'below')\n",
    "        pred_opt = optimal_predictions[metric]\n",
    "        TP_opt, TN_opt, FP_opt, FN_opt = calculate_confusion_matrix(outcomes, pred_opt)\n",
    "        metrics_dict[f'TP_{metric}'] = int(TP_opt)\n",
    "        metrics_dict[f'TN_{metric}'] = int(TN_opt)\n",
    "        metrics_dict[f'FP_{metric}'] = int(FP_opt)\n",
    "        metrics_dict[f'FN_{metric}'] = int(FN_opt)\n",
    "    return_values = [metrics_dict]\n",
    "    for metric in metrics:\n",
    "        return_values.append(metrics_data[metric])\n",
    "    return tuple(return_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_probs(event_mask, condition_mask, bootstrap_replicates, alpha, seed_offset):\n",
    "    if np.sum(condition_mask) > 0:\n",
    "        p_event_given_cond, p_lower, p_upper, _ = bootstrap_percentile_proportion(\n",
    "            event_mask=event_mask,\n",
    "            condition_mask=condition_mask,\n",
    "            replicates=bootstrap_replicates,\n",
    "            seed=seed_offset,\n",
    "            alpha=alpha\n",
    "        )\n",
    "        return float(p_event_given_cond), float(p_lower), float(p_upper)\n",
    "    return np.nan, np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probability_analysis(results, params, scenario, model, metrics, bootstrap_replicates=1000, confidence_level=0.95):\n",
    "    coexistence_mask = np.asarray(results['coexistence'], dtype=bool)\n",
    "    competitive_exclusion_mask = np.asarray(results['competitive_exclusion'], dtype=bool)\n",
    "    has_rare_species = np.asarray(results['has_rare_species'], dtype=bool)\n",
    "    valid_mask = np.asarray(results['valid_outcome'], dtype=bool)\n",
    "    n_total = int(len(coexistence_mask))\n",
    "    alpha = 1.0 - float(confidence_level)\n",
    "    prob_results = {\n",
    "        'model': model,\n",
    "        'scenario': scenario,\n",
    "        'n_total': n_total,\n",
    "        'bootstrap_replicates': int(bootstrap_replicates),\n",
    "        'confidence_level': float(confidence_level),\n",
    "        'coexistence_count': int(np.sum(coexistence_mask)),\n",
    "        'competitive_exclusion_count': int(np.sum(competitive_exclusion_mask))\n",
    "    }\n",
    "    if np.sum(valid_mask) > 0:\n",
    "        p_val, p_low, p_up = compute_conditional_probs(coexistence_mask, valid_mask, bootstrap_replicates, alpha, 0)\n",
    "        prob_results['P_coexistence_given_valid'] = p_val\n",
    "        prob_results['P_coexistence_given_valid_lower'] = p_low\n",
    "        prob_results['P_coexistence_given_valid_upper'] = p_up\n",
    "    if scenario == 'general':\n",
    "        base_mask = valid_mask\n",
    "        seed_base = 1\n",
    "    elif scenario == 'rarity':\n",
    "        base_mask = valid_mask & has_rare_species\n",
    "        seed_base = 10\n",
    "    else:\n",
    "        raise ValueError(\"Unknown scenario: %s\" % scenario)\n",
    "    if np.sum(base_mask) == 0:\n",
    "        return prob_results\n",
    "    for metric_name in metrics:\n",
    "        metric_vals = np.asarray(results[metric_name], dtype=float)\n",
    "        metric_defined_mask = (~np.isnan(metric_vals)) & base_mask\n",
    "        metric_neg_mask = (metric_vals < 0) & metric_defined_mask\n",
    "        metric_pos_mask = (metric_vals >= 0) & metric_defined_mask\n",
    "        p_val, p_low, p_up = compute_conditional_probs(coexistence_mask, metric_neg_mask, bootstrap_replicates, alpha, seed_base)\n",
    "        prob_results[f'P_coexistence_given_{metric_name}_negative'] = p_val\n",
    "        prob_results[f'P_coexistence_given_{metric_name}_negative_lower'] = p_low\n",
    "        prob_results[f'P_coexistence_given_{metric_name}_negative_upper'] = p_up\n",
    "        p_val, p_low, p_up = compute_conditional_probs(coexistence_mask, metric_pos_mask, bootstrap_replicates, alpha, seed_base + 1)\n",
    "        prob_results[f'P_coexistence_given_{metric_name}_positive'] = p_val\n",
    "        prob_results[f'P_coexistence_given_{metric_name}_positive_lower'] = p_low\n",
    "        prob_results[f'P_coexistence_given_{metric_name}_positive_upper'] = p_up\n",
    "        coex_metric_defined = coexistence_mask & (~np.isnan(metric_vals)) & base_mask\n",
    "        if np.sum(coex_metric_defined) > 0:\n",
    "            p_val, p_low, p_up = compute_conditional_probs((metric_vals < 0), coex_metric_defined, bootstrap_replicates, alpha, seed_base + 2)\n",
    "            prob_results[f'P_{metric_name}_negative_given_coexistence'] = p_val\n",
    "            prob_results[f'P_{metric_name}_negative_given_coexistence_lower'] = p_low\n",
    "            prob_results[f'P_{metric_name}_negative_given_coexistence_upper'] = p_up\n",
    "            p_val, p_low, p_up = compute_conditional_probs((metric_vals >= 0), coex_metric_defined, bootstrap_replicates, alpha, seed_base + 3)\n",
    "            prob_results[f'P_{metric_name}_positive_given_coexistence'] = p_val\n",
    "            prob_results[f'P_{metric_name}_positive_given_coexistence_lower'] = p_low\n",
    "            prob_results[f'P_{metric_name}_positive_given_coexistence_upper'] = p_up\n",
    "        prob_results[f'{metric_name}_negative_coexistence_count'] = int(np.sum((metric_vals < 0) & coexistence_mask & (~np.isnan(metric_vals)) & base_mask))\n",
    "        prob_results[f'{metric_name}_positive_coexistence_count'] = int(np.sum((metric_vals >= 0) & coexistence_mask & (~np.isnan(metric_vals)) & base_mask))\n",
    "    return prob_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_config(metric_name):\n",
    "    if metric_name.startswith('nu'):\n",
    "        if '_' in metric_name:\n",
    "            parts = metric_name.split('_')\n",
    "            if len(parts) == 2:\n",
    "                base, sub = parts\n",
    "                return f'$\\\\nu_{{{sub}}}$'\n",
    "        return '$\\\\nu$'\n",
    "    elif metric_name in ['CA', 'CE']:\n",
    "        return f'${metric_name}$'\n",
    "    elif metric_name.startswith('ASL'):\n",
    "        if len(metric_name) > 3:\n",
    "            return f'$ASL_{{{metric_name[3:]}}}$'\n",
    "        return '$ASL$'\n",
    "    elif metric_name in ['S1', 'S2', 'N1', 'N2']:\n",
    "        return f'${metric_name[0]}_{{{metric_name[1]}}}$'\n",
    "    return f'${metric_name}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mcc_comparison(metrics_dict, model_name, metrics, scenario):\n",
    "    if metrics_dict is None:\n",
    "        print(f\"No metrics to plot MCC comparison for {scenario} scenario\")\n",
    "        return\n",
    "    auc_values = []\n",
    "    mcc_values = []\n",
    "    valid_metrics = []\n",
    "    for metric in metrics:\n",
    "        auc_val = metrics_dict.get(f'{metric}_auc', np.nan)\n",
    "        mcc_val = metrics_dict.get(f'{metric}_mcc', np.nan)\n",
    "        if not np.isnan(auc_val) and not np.isnan(mcc_val):\n",
    "            auc_values.append(auc_val)\n",
    "            mcc_values.append(mcc_val)\n",
    "            valid_metrics.append(metric)\n",
    "    if not valid_metrics:\n",
    "        return\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    x = np.arange(len(valid_metrics))\n",
    "    width = 0.35\n",
    "    display_names_list = [get_metric_config(metric) for metric in valid_metrics]\n",
    "    auc_max = max(auc_values) if auc_values else 1.0\n",
    "    mcc_max = max(mcc_values) if mcc_values else 1.0\n",
    "    bars_auc = ax1.bar(x - width/2, auc_values, width, color='blue', edgecolor='black', alpha=0.7, label='AUC')\n",
    "    ax1.set_ylabel(\"Area Under Curve (AUC)\", color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.set_ylim(0, auc_max * 1.1)\n",
    "    ax1.set_xlabel(\"Metric\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(display_names_list, ha='center')\n",
    "    ax2 = ax1.twinx()\n",
    "    bars_mcc = ax2.bar(x + width/2, mcc_values, width, color='green', edgecolor='black', alpha=0.7, label='MCC')\n",
    "    ax2.set_ylabel(\"Matthews Correlation Coefficient (MCC)\", color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "    ax2.set_ylim(0, mcc_max * 1.1)\n",
    "    for i, (bar_auc, bar_mcc) in enumerate(zip(bars_auc, bars_mcc)):\n",
    "        height_auc = bar_auc.get_height()\n",
    "        height_mcc = bar_mcc.get_height()\n",
    "        if not np.isnan(height_auc):\n",
    "            ax1.text(bar_auc.get_x() + bar_auc.get_width()/2., height_auc + 0.01, f'{height_auc:.3g}', ha='center', va='bottom', color='blue')\n",
    "        if not np.isnan(height_mcc):\n",
    "            ax2.text(bar_mcc.get_x() + bar_mcc.get_width()/2., height_mcc + 0.01, f'{height_mcc:.3g}', ha='center', va='bottom', color='green')\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('img', exist_ok=True)\n",
    "    filename = f'img/metric_comparison_{model_name}_{scenario}'\n",
    "    fig.savefig(f'{filename}.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(metrics_data_dict, outcomes, model_name, metrics, scenario):\n",
    "    if len(outcomes) == 0:\n",
    "        print(f\"No data to plot ROC curves for {scenario} scenario\")\n",
    "        return\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    for metric in metrics:\n",
    "        if metric not in metrics_data_dict:\n",
    "            continue\n",
    "        metric_data = metrics_data_dict[metric]\n",
    "        if len(metric_data) != len(outcomes):\n",
    "            print(f\"Warning: Metric {metric} data length ({len(metric_data)}) doesn't match outcomes length ({len(outcomes)})\")\n",
    "            continue\n",
    "        valid_mask = ~np.isnan(metric_data)\n",
    "        metric_data_clean = metric_data[valid_mask]\n",
    "        outcomes_clean = outcomes[valid_mask]\n",
    "        if len(metric_data_clean) == 0:\n",
    "            print(f\"Warning: No valid data for metric {metric} after removing NaN values\")\n",
    "            continue\n",
    "        fpr, tpr, _ = roc_curve(outcomes_clean, -metric_data_clean)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr, lw=2, label=f\"{get_metric_config(metric)} (AUC = {roc_auc:.3g})\")\n",
    "    if len(ax.lines) == 0:\n",
    "        print(f\"No ROC curves to plot for {scenario} scenario\")\n",
    "        plt.close(fig)\n",
    "        return\n",
    "    ax.plot([0, 1], [0, 1], color='black', lw=1, linestyle='--')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.0])\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('img', exist_ok=True)\n",
    "    filename = f'img/roc_curves_{model_name}_{scenario}.pdf'\n",
    "    fig.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probability_analysis(prob_results, model_name, metrics, scenario):\n",
    "    for metric in metrics:\n",
    "        has_data = f'P_coexistence_given_{metric}_negative' in prob_results\n",
    "        if not has_data:\n",
    "            continue\n",
    "        config = get_metric_config(metric)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "        coex_given_neg = prob_results[f'P_coexistence_given_{metric}_negative']\n",
    "        coex_given_pos = prob_results.get(f'P_coexistence_given_{metric}_positive', np.nan)\n",
    "        neg_given_coex = prob_results.get(f'P_{metric}_negative_given_coexistence', np.nan)\n",
    "        pos_given_coex = prob_results.get(f'P_{metric}_positive_given_coexistence', np.nan)\n",
    "        coex_given_neg_lower = prob_results.get(f'P_coexistence_given_{metric}_negative_lower', np.nan)\n",
    "        coex_given_neg_upper = prob_results.get(f'P_coexistence_given_{metric}_negative_upper', np.nan)\n",
    "        coex_given_pos_lower = prob_results.get(f'P_coexistence_given_{metric}_positive_lower', np.nan)\n",
    "        coex_given_pos_upper = prob_results.get(f'P_coexistence_given_{metric}_positive_upper', np.nan)\n",
    "        neg_given_coex_lower = prob_results.get(f'P_{metric}_negative_given_coexistence_lower', np.nan)\n",
    "        neg_given_coex_upper = prob_results.get(f'P_{metric}_negative_given_coexistence_upper', np.nan)\n",
    "        pos_given_coex_lower = prob_results.get(f'P_{metric}_positive_given_coexistence_lower', np.nan)\n",
    "        pos_given_coex_upper = prob_results.get(f'P_{metric}_positive_given_coexistence_upper', np.nan)\n",
    "        x = np.arange(2)\n",
    "        width = 0.35\n",
    "        conditions = [f'{config} < 0', f'{config} \\u2265 0']\n",
    "        coex_given_cond = [coex_given_neg, coex_given_pos]\n",
    "        cond_given_coex = [neg_given_coex, pos_given_coex]\n",
    "        bars1 = ax.bar(x - width/2, coex_given_cond, width, label='P(Coexistence | Condition)', edgecolor='black', alpha=0.7)\n",
    "        bars2 = ax.bar(x + width/2, cond_given_coex, width, label='P(Condition | Coexistence)', edgecolor='black', alpha=0.7)\n",
    "        error_bars_data = [\n",
    "            (x[0] - width/2, coex_given_neg, coex_given_neg_lower, coex_given_neg_upper),\n",
    "            (x[1] - width/2, coex_given_pos, coex_given_pos_lower, coex_given_pos_upper),\n",
    "            (x[0] + width/2, neg_given_coex, neg_given_coex_lower, neg_given_coex_upper),\n",
    "            (x[1] + width/2, pos_given_coex, pos_given_coex_lower, pos_given_coex_upper)\n",
    "        ]\n",
    "        for x_pos, val, lower, upper in error_bars_data:\n",
    "            if not np.isnan(lower) and not np.isnan(upper):\n",
    "                ax.errorbar(x_pos, val, yerr=[[val - lower], [upper - val]], fmt='none', ecolor='black', capsize=3, capthick=1)\n",
    "        for i, (bar1, bar2) in enumerate(zip(bars1, bars2)):\n",
    "            height1 = bar1.get_height()\n",
    "            height2 = bar2.get_height()\n",
    "            if not np.isnan(height1):\n",
    "                ax.text(bar1.get_x() + bar1.get_width()/2., height1 + 0.04, f'{height1:.3g}', ha='center', va='bottom')\n",
    "            if not np.isnan(height2):\n",
    "                ax.text(bar2.get_x() + bar2.get_width()/2., height2 + 0.04, f'{height2:.3g}', ha='center', va='bottom')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(conditions, ha='center')\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        plt.tight_layout()\n",
    "        os.makedirs('img', exist_ok=True)\n",
    "        filename = f'img/probability_analysis_{scenario}_{metric}_{model_name}.pdf'\n",
    "        fig.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_pct(value):\n",
    "#     if abs(value - 100) < 0.01:\n",
    "#         return \"100%\"\n",
    "#     else:\n",
    "#         return f\"{value:.3g}%\"\n",
    "\n",
    "def format_prob(value):\n",
    "    if np.isnan(value):\n",
    "        return \"NaN\"\n",
    "    if abs(value - 1.0) < 0.001:\n",
    "        return \"1.000\"\n",
    "    else:\n",
    "        return f\"{value:.3g}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_probability_analysis(prob_results, metrics, scenario):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PROBABILITY ANALYSIS (Coexistence vs Competitive Exclusion)\")\n",
    "    print(f\"Scenario: {scenario.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"\\n1. BASELINE PROBABILITY:\")\n",
    "    base_val = prob_results.get('P_coexistence_given_valid', np.nan)\n",
    "    base_low = prob_results.get('P_coexistence_given_valid_lower', np.nan)\n",
    "    base_up = prob_results.get('P_coexistence_given_valid_upper', np.nan)\n",
    "    print(f\"   P(Coexistence | Valid outcome) = {format_prob(base_val)} \"\n",
    "          f\"[{format_prob(base_low)}, {format_prob(base_up)}]\")\n",
    "    total_valid = prob_results.get('coexistence_count', 0) + prob_results.get('competitive_exclusion_count', 0)\n",
    "    print(f\"   Total valid cases: {total_valid:,}\")\n",
    "    print(f\"   Coexistence cases: {prob_results.get('coexistence_count', 0):,}\")\n",
    "    print(f\"   Competitive exclusion cases: {prob_results.get('competitive_exclusion_count', 0):,}\")\n",
    "    print(f\"\\n2. HYPOTHESIS TESTS (P(Coexistence | Condition)):\")\n",
    "    for metric in metrics:\n",
    "        neg_key = f'P_coexistence_given_{metric}_negative'\n",
    "        pos_key = f'P_coexistence_given_{metric}_positive'\n",
    "        has_data = neg_key in prob_results\n",
    "        if not has_data:\n",
    "            continue\n",
    "        neg_val = prob_results[neg_key]\n",
    "        pos_val = prob_results.get(pos_key, np.nan)\n",
    "        metric_label = get_metric_config(metric)\n",
    "        print(f\"\\n      {metric_label}:\")\n",
    "        print(f\"        P(Coexistence | {metric_label} < 0) = {format_prob(neg_val)}\")\n",
    "        print(f\"        P(Coexistence | {metric_label} \\u2265 0) = {format_prob(pos_val)}\")\n",
    "        if not np.isnan(neg_val) and not np.isnan(pos_val):\n",
    "            diff = neg_val - pos_val\n",
    "            if diff > 0:\n",
    "                print(f\"        -> {metric_label} < 0 predicts {abs(diff):.3g} ({abs(diff)*100:.3g}%) MORE coexistence\")\n",
    "            elif diff < 0:\n",
    "                print(f\"        -> {metric_label} \\u2265 0 predicts {abs(diff):.3g} ({abs(diff)*100:.3g}%) MORE coexistence\")\n",
    "            else:\n",
    "                print(f\"        -> No difference in prediction\")\n",
    "    print(f\"\\n3. COUNTS:\")\n",
    "    for metric in metrics:\n",
    "        neg_key = f'{metric}_negative_coexistence_count'\n",
    "        pos_key = f'{metric}_positive_coexistence_count'\n",
    "        if neg_key in prob_results:\n",
    "            print(f\"      {get_metric_config(metric)} < 0 in coexistence: {prob_results.get(neg_key, 0):,}\")\n",
    "            print(f\"      {get_metric_config(metric)} \\u2265 0 in coexistence: {prob_results.get(pos_key, 0):,}\")\n",
    "    print(f\"\\n{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_analysis(metrics_dict, metrics, scenario):\n",
    "    if metrics_dict is None:\n",
    "        print(f\"No classification metrics for {scenario} scenario\")\n",
    "        return\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"COMPREHENSIVE METRIC ANALYSIS - {scenario.upper()} SCENARIO\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"\\n1. SAMPLE STATISTICS:\")\n",
    "    n_cases = metrics_dict['n_cases']\n",
    "    coexistence_count = metrics_dict['coexistence_count']\n",
    "    competitive_exclusion_count = metrics_dict['competitive_exclusion_count']\n",
    "    coexistence_pct = (coexistence_count / n_cases * 100) if n_cases > 0 else 0\n",
    "    print(f\"   Total cases: {n_cases:,}\")\n",
    "    print(f\"   Coexistence cases: {coexistence_count:,} ({coexistence_pct:.2g}%)\")\n",
    "    print(f\"   Competitive exclusion cases: {competitive_exclusion_count:,} ({100-coexistence_pct:.2g}%)\")\n",
    "    print(f\"\\n2. METRIC COMPARISON (Hypothesis Independent):\")\n",
    "    print(f\"   Which metric best separates classes, regardless of direction?\")\n",
    "    print(f\"   {'Metric':<15} {'AUC':<10} {'MCC':<10} {'Threshold':<12} {'Direction':<10} {'Accuracy':<10}\")\n",
    "    print(f\"   {'-'*15} {'-'*10} {'-'*10} {'-'*12} {'-'*10} {'-'*10}\")\n",
    "    for metric in metrics:\n",
    "        auc_val = metrics_dict.get(f'{metric}_auc', np.nan)\n",
    "        mcc_val = metrics_dict.get(f'{metric}_mcc', np.nan)\n",
    "        threshold = metrics_dict.get(f'{metric}_threshold', 0.0)\n",
    "        direction = metrics_dict.get(f'{metric}_direction', 'below')\n",
    "        acc_val = metrics_dict.get(f'{metric}_accuracy', np.nan)\n",
    "        label = get_metric_config(metric)\n",
    "        auc_str = f'{auc_val:.3g}' if not np.isnan(auc_val) else 'NaN'\n",
    "        mcc_str = f'{mcc_val:.3g}' if not np.isnan(mcc_val) else 'NaN'\n",
    "        threshold_str = f'{threshold:.3g}'\n",
    "        direction_str = f'{direction} threshold'\n",
    "        acc_str = f'{acc_val:.3g}' if not np.isnan(acc_val) else 'NaN'\n",
    "        print(f\"   {label:<15} {auc_str:>9} {mcc_str:>9} {threshold_str:>11} {direction_str:>9} {acc_str:>9}\")\n",
    "    print(f\"\\n3. BEST METRIC ANALYSIS:\")\n",
    "    best_metric = metrics_dict.get('better_metric', 'none')\n",
    "    best_score = metrics_dict.get('better_score', 0.0)\n",
    "    if best_metric != 'none':\n",
    "        best_direction = metrics_dict.get(f'{best_metric}_direction', 'below')\n",
    "        best_threshold = metrics_dict.get(f'{best_metric}_threshold', 0.0)\n",
    "        best_auc = metrics_dict.get(f'{best_metric}_auc', 0.5)\n",
    "        best_mcc = metrics_dict.get(f'{best_metric}_mcc', 0.0)\n",
    "        print(f\"   Best metric (by AUC): {get_metric_config(best_metric)}\")\n",
    "        print(f\"   AUC: {best_auc:.3g}\")\n",
    "        print(f\"   MCC: {best_mcc:.3g}\")\n",
    "        print(f\"   Optimal rule: {get_metric_config(best_metric)} {best_direction} {best_threshold:.3g}\")\n",
    "        auc_values = []\n",
    "        for metric in metrics:\n",
    "            if metric != best_metric:\n",
    "                auc_val = metrics_dict.get(f'{metric}_auc', 0.5)\n",
    "                auc_values.append(auc_val)\n",
    "        if auc_values:\n",
    "            second_best = max(auc_values)\n",
    "            improvement = best_auc - second_best\n",
    "            print(f\"   AUC improvement over second best: {improvement:.3g}\")\n",
    "    print(f\"\\n4. HYPOTHESIS ALIGNMENT CHECK:\")\n",
    "    for metric in metrics:\n",
    "        direction = metrics_dict.get(f'{metric}_direction', 'below')\n",
    "        threshold = metrics_dict.get(f'{metric}_threshold', 0.0)\n",
    "        label = get_metric_config(metric)\n",
    "        if direction == 'below' and abs(threshold) < 0.1:\n",
    "            alignment = \"Aligns with hypothesis\"\n",
    "        else:\n",
    "            alignment = f\"Differs: {direction} threshold {threshold:.3g}\"\n",
    "        print(f\"   {label}: {alignment}\")\n",
    "    print(f\"\\n{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(results, params, prob_results, metrics_dict, model_name, scenario, metrics):\n",
    "    df_results = pd.DataFrame({\n",
    "        'r1': params[:, 0],\n",
    "        'r2': params[:, 1],\n",
    "        'a11': params[:, 2],\n",
    "        'a22': params[:, 3],\n",
    "        'a12': params[:, 4],\n",
    "        'a21': params[:, 5],\n",
    "        'valid_outcome': results['valid_outcome'],\n",
    "        'coexistence': results['coexistence'],\n",
    "        'competitive_exclusion': results['competitive_exclusion'],\n",
    "        'has_rare_species': results['has_rare_species']\n",
    "    })\n",
    "    for metric in metrics:\n",
    "        df_results[metric] = results[metric]\n",
    "    os.makedirs('csv', exist_ok=True)\n",
    "    results_filename = f'csv/results_{model_name}_{scenario}.csv'\n",
    "    df_results.to_csv(results_filename, index=False)\n",
    "    if prob_results:\n",
    "        prob_df = pd.DataFrame([prob_results])\n",
    "        prob_filename = f'csv/probability_{model_name}_{scenario}.csv'\n",
    "        prob_df.to_csv(prob_filename, index=False)\n",
    "    if metrics_dict:\n",
    "        metrics_dict['model'] = model_name\n",
    "        metrics_dict['scenario'] = scenario\n",
    "        metrics_df = pd.DataFrame([metrics_dict])\n",
    "        metrics_filename = f'csv/classification_metrics_{model_name}_{scenario}.csv'\n",
    "        metrics_df.to_csv(metrics_filename, index=False)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunk(params_chunk, model, metrics):\n",
    "    n = len(params_chunk)\n",
    "    results = {\n",
    "        'valid_outcome': np.zeros(n, dtype=bool),\n",
    "        'coexistence': np.zeros(n, dtype=bool),\n",
    "        'competitive_exclusion': np.zeros(n, dtype=bool),\n",
    "        'has_rare_species': np.zeros(n, dtype=bool)\n",
    "    }\n",
    "    for metric in metrics:\n",
    "        results[metric] = np.zeros(n, dtype=float)\n",
    "    for i in range(n):\n",
    "        res = analyze_parameter_set(params_chunk[i], model, metrics)\n",
    "        results['valid_outcome'][i] = res['valid_outcome']\n",
    "        results['coexistence'][i] = res['coexistence']\n",
    "        results['competitive_exclusion'][i] = res['competitive_exclusion']\n",
    "        results['has_rare_species'][i] = res['has_rare_species']\n",
    "        for metric in metrics:\n",
    "            results[metric][i] = res[metric]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel_analysis(model, metrics, n_samples=10000, n_jobs=-1):\n",
    "    print(f\"Generating {n_samples:,} parameter samples...\")\n",
    "    params = sample_parameters_lhs(n_samples=n_samples)\n",
    "    chunk_size = max(1, n_samples // n_jobs)\n",
    "    print(f\"Running analysis with {n_jobs} parallel jobs (chunk size: {chunk_size:,})...\")\n",
    "    chunks = []\n",
    "    for i in range(0, n_samples, chunk_size):\n",
    "        chunks.append(params[i:i + chunk_size])\n",
    "    results_list = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(analyze_chunk)(chunk, model, metrics) for chunk in chunks\n",
    "    )\n",
    "    combined_results = {\n",
    "        'valid_outcome': np.concatenate([r['valid_outcome'] for r in results_list]),\n",
    "        'coexistence': np.concatenate([r['coexistence'] for r in results_list]),\n",
    "        'competitive_exclusion': np.concatenate([r['competitive_exclusion'] for r in results_list]),\n",
    "        'has_rare_species': np.concatenate([r['has_rare_species'] for r in results_list])\n",
    "    }\n",
    "    for metric in metrics:\n",
    "        combined_results[metric] = np.concatenate([r[metric] for r in results_list])\n",
    "    return combined_results, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(model_name, metrics, scenario, n_samples=10000, n_jobs=-1, bootstrap_replicates=1000, confidence_level=0.95):\n",
    "    metrics_dict = None\n",
    "    np.random.seed(1234)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"COMPREHENSIVE ANALYSIS FOR {model_name.upper()} MODEL\")\n",
    "    print(f\"Scenario: {scenario.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    if model_name == 'ricker':\n",
    "        metrics_for_model = [m for m in metrics if m not in ('nu_CA', 'nu_CE')]\n",
    "    else:\n",
    "        metrics_for_model = list(metrics)\n",
    "    results, params = run_parallel_analysis(\n",
    "        model=model_name,\n",
    "        metrics=metrics_for_model,\n",
    "        n_samples=n_samples,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    print(f\"\\nANALYSIS COMPLETED\")\n",
    "    print(f\"Total samples: {n_samples:.3g}\")\n",
    "    valid_count = np.sum(results['valid_outcome'])\n",
    "    print(f\"Valid outcomes (coexistence or exclusion): {valid_count:.3g} ({valid_count/n_samples*100:.3g}%)\")\n",
    "    coexist_count = np.sum(results['coexistence'])\n",
    "    excl_count = np.sum(results['competitive_exclusion'])\n",
    "    if valid_count > 0:\n",
    "        print(f\"Coexistence cases: {coexist_count:.3g} ({coexist_count/valid_count*100:.3g}% of valid)\")\n",
    "        print(f\"Competitive exclusion cases: {excl_count:.3g} ({excl_count/valid_count*100:.3g}% of valid)\")\n",
    "    else:\n",
    "        print(f\"Coexistence cases: {coexist_count:.3g}\")\n",
    "        print(f\"Competitive exclusion cases: {excl_count:.3g}\")\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"RUNNING COMPREHENSIVE PROBABILITY ANALYSIS...\")\n",
    "    print(f\"Bootstrap replicates: {bootstrap_replicates:.3g}\")\n",
    "    prob_results = compute_probability_analysis(\n",
    "        results, params, scenario, model=model_name, metrics=metrics_for_model,\n",
    "        bootstrap_replicates=bootstrap_replicates,\n",
    "        confidence_level=confidence_level\n",
    "    )\n",
    "    print_probability_analysis(prob_results, metrics_for_model, scenario)\n",
    "    plot_probability_analysis(prob_results, model_name, metrics_for_model, scenario)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"RUNNING CLASSIFICATION ANALYSIS...\")\n",
    "    classification_results = calculate_comparison_metrics(\n",
    "        results, params, metrics_for_model, scenario, bootstrap_replicates\n",
    "    )\n",
    "    if classification_results and classification_results[0] is not None:\n",
    "        metrics_dict = classification_results[0]\n",
    "        metric_data_arrays = classification_results[1:]\n",
    "        print_classification_analysis(metrics_dict, metrics_for_model, scenario)\n",
    "        metrics_data_dict = dict(zip(metrics_for_model, metric_data_arrays))\n",
    "        if scenario == 'general':\n",
    "            mask = results['valid_outcome']\n",
    "        elif scenario == 'rarity':\n",
    "            mask = results['valid_outcome'] & results['has_rare_species']\n",
    "        else:\n",
    "            mask = results['valid_outcome']\n",
    "        outcomes = np.asarray(results['coexistence'][mask])\n",
    "        if len(outcomes) > 0:\n",
    "            min_len = min(len(outcomes), len(metrics_data_dict[metrics_for_model[0]]))\n",
    "            outcomes = outcomes[:min_len]\n",
    "            for metric in metrics_for_model:\n",
    "                metrics_data_dict[metric] = metrics_data_dict[metric][:min_len]\n",
    "            plot_roc_curves(metrics_data_dict, outcomes, model_name, metrics_for_model, scenario)\n",
    "        plot_mcc_comparison(metrics_dict, model_name, metrics_for_model, scenario)\n",
    "    else:\n",
    "        metrics_data_dict = None\n",
    "    save_results_to_csv(results, params, prob_results,\n",
    "                        metrics_dict if classification_results else None,\n",
    "                        model_name, scenario, metrics_for_model)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ANALYSIS FOR {model_name.upper()} - {scenario.upper()} COMPLETE!\")\n",
    "    print(f\"{'='*50}\")\n",
    "    return {\n",
    "        'results': results,\n",
    "        'params': params,\n",
    "        'prob_results': prob_results,\n",
    "        'metrics_dict': metrics_dict\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    n_samples = 20000\n",
    "    n_jobs = -1\n",
    "    bootstrap_replicates = int(0.1*n_samples)\n",
    "    confidence_level = 0.95\n",
    "    models = ['bevertonHolt'] # , 'ricker'\n",
    "    metrics = ['nu', 'nu_CE'] # , 'nu_CA', 'nu_ASL', 'nu_a'\n",
    "    scenarios = ['rarity'] # , 'general'\n",
    "    print(\"=\"*50)\n",
    "    print(\"COEXISTENCE vs COMPETITIVE EXCLUSION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Number of samples: {n_samples:,}\")\n",
    "    print(f\"Bootstrap replicates: {bootstrap_replicates:,}\")\n",
    "    print(f\"Confidence level: {confidence_level:.0%}\")\n",
    "    print(f\"Parallel jobs: {n_jobs if n_jobs > 0 else 'all cores'}\")\n",
    "    print(f\"Scenarios: {', '.join(scenarios)}\")\n",
    "    print(\"=\"*50)\n",
    "    all_results = {}\n",
    "    for model in models:\n",
    "        model_results = {}\n",
    "        for scenario in scenarios:\n",
    "            scenario_results = analyze_model(\n",
    "                model_name=model,\n",
    "                metrics=metrics,\n",
    "                scenario=scenario,\n",
    "                n_samples=n_samples,\n",
    "                n_jobs=n_jobs,\n",
    "                bootstrap_replicates=bootstrap_replicates,\n",
    "                confidence_level=confidence_level\n",
    "            )\n",
    "            model_results[scenario] = scenario_results\n",
    "        all_results[model] = model_results\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for model, model_results in all_results.items():\n",
    "        print(f\"\\n{model.upper()} Model:\")\n",
    "        for scenario in scenarios:\n",
    "            if scenario in model_results:\n",
    "                prob = model_results[scenario]['prob_results']\n",
    "                metrics_dict = model_results[scenario].get('metrics_dict', {})\n",
    "                print(f\"  {scenario.upper()} scenario:\")\n",
    "                print(f\"    P(Coexistence | Valid outcome): {prob.get('P_coexistence_given_valid', np.nan):.3g}\")\n",
    "                if metrics_dict:\n",
    "                    for metric in metrics:\n",
    "                        print(f\"    {get_metric_config(metric)} MCC: {metrics_dict.get(f'{metric}_mcc', 0):.3g}\")\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"ANALYSIS COMPLETE! All results saved to 'csv/' directory.\")\n",
    "    print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "COEXISTENCE vs COMPETITIVE EXCLUSION ANALYSIS\n",
      "==================================================\n",
      "Number of samples: 20,000\n",
      "Bootstrap replicates: 2,000\n",
      "Confidence level: 95%\n",
      "Parallel jobs: all cores\n",
      "Scenarios: rarity\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "COMPREHENSIVE ANALYSIS FOR BEVERTONHOLT MODEL\n",
      "Scenario: RARITY\n",
      "==================================================\n",
      "Generating 20,000 parameter samples...\n",
      "Running analysis with -1 parallel jobs (chunk size: 1)...\n",
      "\n",
      "ANALYSIS COMPLETED\n",
      "Total samples: 2e+04\n",
      "Valid outcomes (coexistence or exclusion): 1.66e+04 (83.2%)\n",
      "Coexistence cases: 3.33e+03 (20% of valid)\n",
      "Competitive exclusion cases: 1.33e+04 (80% of valid)\n",
      "\n",
      "==================================================\n",
      "RUNNING COMPREHENSIVE PROBABILITY ANALYSIS...\n",
      "Bootstrap replicates: 2e+03\n",
      "\n",
      "==================================================\n",
      "PROBABILITY ANALYSIS (Coexistence vs Competitive Exclusion)\n",
      "Scenario: RARITY\n",
      "==================================================\n",
      "\n",
      "1. BASELINE PROBABILITY:\n",
      "   P(Coexistence | Valid outcome) = 0.2 [0.194, 0.206]\n",
      "   Total valid cases: 16,644\n",
      "   Coexistence cases: 3,330\n",
      "   Competitive exclusion cases: 13,314\n",
      "\n",
      "2. HYPOTHESIS TESTS (P(Coexistence | Condition)):\n",
      "\n",
      "      $\\nu$:\n",
      "        P(Coexistence | $\\nu$ < 0) = 0.198\n",
      "        P(Coexistence | $\\nu$  0) = 0.0768\n",
      "        -> $\\nu$ < 0 predicts 0.121 (12.1%) MORE coexistence\n",
      "\n",
      "      $\\nu_{CE}$:\n",
      "        P(Coexistence | $\\nu_{CE}$ < 0) = 1.000\n",
      "        P(Coexistence | $\\nu_{CE}$  0) = 0.0893\n",
      "        -> $\\nu_{CE}$ < 0 predicts 0.911 (91.1%) MORE coexistence\n",
      "\n",
      "3. COUNTS:\n",
      "      $\\nu$ < 0 in coexistence: 866\n",
      "      $\\nu$  0 in coexistence: 815\n",
      "      $\\nu_{CE}$ < 0 in coexistence: 375\n",
      "      $\\nu_{CE}$  0 in coexistence: 1,306\n",
      "\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "RUNNING CLASSIFICATION ANALYSIS...\n",
      "\n",
      "==================================================\n",
      "COMPREHENSIVE METRIC ANALYSIS - RARITY SCENARIO\n",
      "==================================================\n",
      "\n",
      "1. SAMPLE STATISTICS:\n",
      "   Total cases: 14,995\n",
      "   Coexistence cases: 1,681 (11%)\n",
      "   Competitive exclusion cases: 13,314 (89%)\n",
      "\n",
      "2. METRIC COMPARISON (Hypothesis Independent):\n",
      "   Which metric best separates classes, regardless of direction?\n",
      "   Metric          AUC        MCC        Threshold    Direction  Accuracy  \n",
      "   --------------- ---------- ---------- ------------ ---------- ----------\n",
      "   $\\nu$               0.678      0.25       -4.03 below threshold     0.863\n",
      "   $\\nu_{CE}$          0.769     0.428       0.362 below threshold     0.911\n",
      "\n",
      "3. BEST METRIC ANALYSIS:\n",
      "   Best metric (by AUC): $\\nu_{CE}$\n",
      "   AUC: 0.769\n",
      "   MCC: 0.428\n",
      "   Optimal rule: $\\nu_{CE}$ below 0.362\n",
      "   AUC improvement over second best: 0.091\n",
      "\n",
      "4. HYPOTHESIS ALIGNMENT CHECK:\n",
      "   $\\nu$: Differs: below threshold -4.03\n",
      "   $\\nu_{CE}$: Differs: below threshold 0.362\n",
      "\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "ANALYSIS FOR BEVERTONHOLT - RARITY COMPLETE!\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "FINAL SUMMARY\n",
      "==================================================\n",
      "\n",
      "BEVERTONHOLT Model:\n",
      "  RARITY scenario:\n",
      "    P(Coexistence | Valid outcome): 0.2\n",
      "    $\\nu$ MCC: 0.25\n",
      "    $\\nu_{CE}$ MCC: 0.428\n",
      "\n",
      "==================================================\n",
      "ANALYSIS COMPLETE! All results saved to 'csv/' directory.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def time_simul(r1, r2, a11, a22, a12, a21, y01=5.0, y02=5.0, eps=1e-3):\n",
    "    y1 = np.array([y01], dtype=np.float64)\n",
    "    y2 = np.array([y02], dtype=np.float64)\n",
    "    stop_run = False\n",
    "    i = 0\n",
    "    while not stop_run and i < 1000:\n",
    "        denom1 = 1 + a11 * y1[i] + a12 * y2[i]\n",
    "        denom2 = 1 + a22 * y2[i] + a21 * y1[i]\n",
    "        per_cap1 = r1 / denom1\n",
    "        per_cap2 = r2 / denom2\n",
    "        new_y1 = y1[i] * per_cap1\n",
    "        new_y2 = y2[i] * per_cap2\n",
    "        y1 = np.append(y1, new_y1)\n",
    "        y2 = np.append(y2, new_y2)\n",
    "        if i >= 1:\n",
    "            if (abs(y1[-1] - y1[-2]) < eps and abs(y2[-1] - y2[-2]) < eps):\n",
    "                stop_run = True\n",
    "        i += 1\n",
    "    return y1, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def resolve_saddle_with_simulation(r1, r2, a11, a22, a12, a21, y01=5.0, y02=5.0):\n",
    "    try:\n",
    "        y1, y2 = time_simul(r1, r2, a11, a22, a12, a21, y01, y02)\n",
    "        final_N1 = y1[-1]\n",
    "        final_N2 = y2[-1]\n",
    "        # Check if both species persist\n",
    "        if final_N1 > 1e-6 and final_N2 > 1e-6:\n",
    "            return 'coexistence'\n",
    "        elif final_N1 > 1e-6 and final_N2 <= 1e-6:\n",
    "            return 'extinction_species2'  # Species 1 wins\n",
    "        elif final_N1 <= 1e-6 and final_N2 > 1e-6:\n",
    "            return 'extinction_species1'  # Species 2 wins\n",
    "        else:\n",
    "            return 'extinction_both'\n",
    "    except:\n",
    "        return 'simulation_error'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
