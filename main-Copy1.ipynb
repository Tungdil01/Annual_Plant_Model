{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code aims to modify the analysis of Yenni et al. (2012):\n",
    "#### - has the option to keep the filter S1 >= 1 & S2 >= 1 or remove it\n",
    "#### - does not truncate the values\n",
    "#### - counts the number of coexistence cases\n",
    "#### - includes Cushing et al. (2004) analytical results\n",
    "\n",
    "#### their original code: https://github.com/gmyenni/RareStabilizationSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "# import traceback; traceback.print_exc()\n",
    "# import matplotlib.ticker as mticker\n",
    "# from scipy.stats import ttest_ind, mannwhitneyu, shapiro\n",
    "# from joblib import Parallel, delayed\n",
    "# from tabulate import tabulate\n",
    "# import pymc as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyN_function.r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@jit\n",
    "def analyN(r1, r2, a11, a12, a21, a22):# time_simul\n",
    "    y01, y02 =5, 5\n",
    "    y1 = np.array([y01], dtype=np.float64)\n",
    "    y2 = np.array([y02], dtype=np.float64)\n",
    "    i, stopRun = 0, 0\n",
    "    count_PGR1, count_PGR2 = 0, 0\n",
    "    while stopRun == 0:  # stops the simulation when the dynamics converges\n",
    "        per_cap1 = r1 / (1 + a11 * y1[i] + a12 * y2[i])\n",
    "        per_cap2 = r2 / (1 + a22 * y2[i] + a21 * y1[i])\n",
    "        # if per_cap1 > per_cap2:\n",
    "        #     count_PGR1 += 1\n",
    "        # elif per_cap2 > per_cap1:\n",
    "        #     count_PGR2 += 1\n",
    "        y1 = np.append(y1, y1[i] * per_cap1)\n",
    "        y2 = np.append(y2, y2[i] * per_cap2)        \n",
    "        if np.abs(y1[-1] - y1[-2]) < 1.0e-6 and np.abs(y2[-1] - y2[-2]) < 1.0e-6:\n",
    "            stopRun = 1\n",
    "        i += 1\n",
    "        if i > 10000:\n",
    "            break\n",
    "    return y1[-1], y2[-1]#, count_PGR1, count_PGR2\n",
    "\n",
    "def analyN2(r1, r2, a11, a12, a21, a22): # equilibrium populations\n",
    "    N1 = (r1 - 1 - (a12 / a22) * (r2 - 1)) / (a11 - a21 * a12 / a22)\n",
    "    N2 = (r2 - 1 - (a21 / a11) * (r1 - 1)) / (a22 - a21 * a12 / a11)\n",
    "    if np.isinf(N1) or np.isinf(N2) or np.isnan(N1) or np.isnan(N2):\n",
    "        initialNsp1 = 0\n",
    "        initialNsp2 = 0\n",
    "        N = np.zeros((100, 2))\n",
    "        N[0, :] = [initialNsp1, initialNsp2]   \n",
    "        for i in range(1, 100):\n",
    "            N[i, 0] = max((r1 - 1 - a12 * N[i-1, 1]) / a11, 0)\n",
    "            N[i, 1] = max((r2 - 1 - a21 * N[i-1, 0]) / a22, 0)\n",
    "        N1 = np.mean(N[:, 0])\n",
    "        N2 = np.mean(N[:, 1])\n",
    "    if N1 < 0 and N2 >= 0:\n",
    "        N1 = 0\n",
    "        N2 = (r2 - 1) / a22\n",
    "    elif N2 < 0 and N1 >= 0:\n",
    "        N2 = 0\n",
    "        N1 = (r1 - 1) / a11\n",
    "    return N1, N2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_equation(r1, r2, a11, a12, a21, a22, solver='numeric'):\n",
    "    if solver == 'analyN': # Analytical approximation for equilibrium populations\n",
    "        denominator1 = a11 - (a21 * a12) / a22\n",
    "        denominator2 = a22 - (a21 * a12) / a11\n",
    "        N1 = (r1 - 1 - (a12 / a22) * (r2 - 1)) / denominator1 if denominator1 != 0 else np.nan\n",
    "        N2 = (r2 - 1 - (a21 / a11) * (r1 - 1)) / denominator2 if denominator2 != 0 else np.nan\n",
    "        if np.isinf(N1) or np.isinf(N2) or np.isnan(N1) or np.isnan(N2):\n",
    "            initialNsp1 = 0\n",
    "            initialNsp2 = 0\n",
    "            N = np.zeros((100, 2))\n",
    "            N[0, :] = [initialNsp1, initialNsp2]   \n",
    "            for i in range(1, 100):\n",
    "                N[i, 0] = max((r1 - 1 - a12 * N[i-1, 1]) / a11, 0)\n",
    "                N[i, 1] = max((r2 - 1 - a21 * N[i-1, 0]) / a22, 0)\n",
    "            N1 = np.mean(N[:, 0])\n",
    "            N2 = np.mean(N[:, 1])\n",
    "        if N1 < 0 and N2 >= 0:\n",
    "            N1, N2 = 0.0, (r2 - 1) / a22 if a22 != 0 else 0.0\n",
    "        elif N2 < 0 and N1 >= 0:\n",
    "            N1, N2 = (r1 - 1) / a11 if a11 != 0 else 0.0, 0.0\n",
    "        elif N1 < 0 and N2 < 0:\n",
    "            N1, N2 = 0.0, 0.0\n",
    "        return N1, N2\n",
    "    elif solver == 'numeric': # Numerical simulation with convergence check\n",
    "        y1 = np.array([5.0], dtype=np.float64)\n",
    "        y2 = np.array([5.0], dtype=np.float64)\n",
    "        stop_run = False\n",
    "        i = 0\n",
    "        while not stop_run and i < 10000:\n",
    "            denom1 = 1 + a11 * y1[i] + a12 * y2[i]\n",
    "            denom2 = 1 + a22 * y2[i] + a21 * y1[i]\n",
    "            per_cap1 = r1 / denom1\n",
    "            per_cap2 = r2 / denom2\n",
    "            new_y1 = y1[i] * per_cap1\n",
    "            new_y2 = y2[i] * per_cap2\n",
    "            y1 = np.append(y1, new_y1)\n",
    "            y2 = np.append(y2, new_y2)\n",
    "            if i >= 1:\n",
    "                if (abs(y1[-1] - y1[-2]) < 1e-6 and abs(y2[-1] - y2[-2]) < 1e-6):\n",
    "                    stop_run = True\n",
    "            i += 1\n",
    "        return y1[-1], y2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_counts_test(filtered_data, print_on=False):\n",
    "    print('\\nPGR Statistics:\\n')\n",
    "    count_PGR1, count_PGR2 = [], []\n",
    "    for _, row in filtered_data.iterrows():\n",
    "        PGR1, PGR2 = getPCG(row['r1'], row['r2'], row['a11'], row['a12'], row['a21'], row['a22'], row['N1'], row['N2'])\n",
    "        count_PGR1.append(PGR1)\n",
    "        count_PGR2.append(PGR2)\n",
    "    count_PGR1 = pd.Series(count_PGR1).dropna()\n",
    "    count_PGR2 = pd.Series(count_PGR2).dropna()\n",
    "    if count_PGR1.empty or count_PGR2.empty:\n",
    "        print(\"No valid PGR data.\")\n",
    "        return None\n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        \"mean_PGR1\": count_PGR1.mean(),\n",
    "        \"mean_PGR2\": count_PGR2.mean(),\n",
    "        \"std_PGR1\": count_PGR1.std(ddof=1),\n",
    "        \"std_PGR2\": count_PGR2.std(ddof=1),\n",
    "        \"median_PGR1\": count_PGR1.median(),\n",
    "        \"median_PGR2\": count_PGR2.median()\n",
    "    }\n",
    "    print(f\"PGR1 Mean \\u00B1 SD: {stats['mean_PGR1']:.2g} \\u00B1 {stats['std_PGR1']:.2g}\")\n",
    "    print(f\"PGR2 Mean \\u00B1 SD: {stats['mean_PGR2']:.2g} \\u00B1 {stats['std_PGR2']:.2g}\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getNFD.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def getPCG(r1, r2, a11, a12, a21, a22, N1, N2): # Per capita growth rate calculation\n",
    "    newN1 = r1 * N1 / (1 + a11 * N1 + a12 * N2) if N1 > 0 else np.nan\n",
    "    newN2 = r2 * N2 / (1 + a22 * N2 + a21 * N1) if N2 > 0 else np.nan\n",
    "    PGR1 = np.log(newN1) - np.log(N1) if N1 > 0 else np.nan\n",
    "    PGR2 = np.log(newN2) - np.log(N2) if N2 > 0 else np.nan\n",
    "    return PGR1, PGR2\n",
    "\n",
    "@jit\n",
    "def calculate_metrics(r1, r2, a11, a12, a21, a22, N1, N2, extinc_crit_1=True):\n",
    "    S1 = r2 / (1 + (a12 / a22) * (r2 - 1))\n",
    "    S2 = r1 / (1 + (a21 / a11) * (r1 - 1))\n",
    "    FE1, FE2 = r1 / r2, r2 / r1 # Fitness equivalence\n",
    "    Asy = S1 - S2 # Asymmetry\n",
    "    Rare = 0 if N1 == 0 and N2 == 0 else N1 / (N1 + N2)\n",
    "    # Calculating covariance for SoS\n",
    "    x = np.array([N1, N2])\n",
    "    y_sos = np.array([S1, S2])\n",
    "    cor_matrix_sos = np.cov(x, y_sos)\n",
    "    cor_sos = cor_matrix_sos[0, 1] # Extracting the correlation between N and SoS\n",
    "    Rank = 0 if N1 == 0 and N2 == 0 else (2 if N1 / (N1 + N2) <= 0.25 else 1)\n",
    "    # Equilibrium points\n",
    "    E1 = (r1 - 1) / a11\n",
    "    E2 = (r2 - 1) / a22\n",
    "    P = (r1 - 1) / a12\n",
    "    Q = (r2 - 1) / a21\n",
    "    # Calculate conditions for A, B, C, D\n",
    "    A = P > E2 and E1 > Q\n",
    "    B = E2 > P and Q > E1\n",
    "    C = P > E2 and Q > E1\n",
    "    D = E2 > P and E1 > Q\n",
    "    # Call getPCG to calculate PGR1 and PGR2\n",
    "    PGR1, PGR2 = getPCG(r1, r2, a11, a12, a21, a22, N1, N2)\n",
    "    if extinc_crit_1:\n",
    "        Coexist = 0 if N1 < 1 or N2 < 1 else 1\n",
    "    else:\n",
    "        Coexist = 0 if N1 < 1.0e-6 or N2 < 1.0e-6 else 1\n",
    "    return {\"FE1\": FE1, \"S1\": S1, \"FE2\": FE2, \"S2\": S2, \"Rank\": Rank, \"Coexist\": Coexist, \"Asy\": Asy, \"cor_sos\": cor_sos, \"Rare\": Rare, \"PGR1\": PGR1, \"PGR2\": PGR2, \"A\": A, \"B\": B, \"C\": C, \"D\": D}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# annualplant_2spp_det_par.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(pars):\n",
    "    # Defines frequency-dependent parameters\n",
    "    if pars == 'r_code': # Their R code\n",
    "         r1_v = np.arange(10, 21, 1)\n",
    "         r2_v = np.arange(10, 21, 1)\n",
    "         a11_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1, 1.5, 2, 2.5, 3])\n",
    "         a12_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "         a21_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "         a22_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "    elif pars == 'table1': # Reproduce their Table 1\n",
    "        r1_v = np.arange(15, 21, 1)\n",
    "        r2_v = np.arange(15, 21, 1)\n",
    "        a11_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1, 1.5, 2, 2.5, 3])\n",
    "        a12_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "        a21_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "        a22_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "    elif pars == 'paper': # They describe in the paper\n",
    "         r1_v = np.arange(15, 21, 1)\n",
    "         r2_v = np.arange(11, 21, 1)\n",
    "         a11_v = np.array([0.7, 0.3, 0.5, 0.7, 0.9, 1, 1.5, 2, 2.5, 3])\n",
    "         a12_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "         a21_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "         a22_v = np.array([0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "    else: # minimal: Reduced set of parameters\n",
    "        r1_v = np.array([15, 17, 18, 20])\n",
    "        r2_v = np.array([15, 17, 18, 20])\n",
    "        a11_v = np.array([0.1, 1, 3])\n",
    "        a12_v = np.array([0.1, 0.5, 1])\n",
    "        a21_v = np.array([0.1, 0.5, 1])\n",
    "        a22_v = np.array([0.1, 0.5, 1])\n",
    "    # Generate all combinations of parameters using NumPy's meshgrid\n",
    "    mesh = np.array(np.meshgrid(r1_v, r2_v, a11_v, a12_v, a21_v, a22_v)).T.reshape(-1, 6)\n",
    "    return mesh\n",
    "\n",
    "def Sim(k, mesh_row, extinc_crit_1=False, solver='numeric'):\n",
    "    start_time = time.time()\n",
    "    r1, r2, a11, a12, a21, a22 = mesh_row\n",
    "    N1, N2 = difference_equation(r1, r2, a11, a12, a21, a22, solver=solver)\n",
    "    metrics = calculate_metrics(r1, r2, a11, a12, a21, a22, N1, N2, extinc_crit_1)\n",
    "    execution_time = time.time() - start_time\n",
    "    return {**metrics, \"N1\": N1, \"N2\": N2, \"r1\": r1, \"r2\": r2, \"a11\": a11, \"a12\": a12, \"a21\": a21, \"a22\": a22}\n",
    "\n",
    "def postprocess_results(results, outfile):\n",
    "    column_order = ['r1', 'r2', 'a11', 'a12', 'a21', 'a22', 'N1', 'N2', 'FE1', 'S1', 'FE2', 'S2', 'Rank', 'Coexist', 'Asy', 'cor_sos', 'Rare', 'PGR1', 'PGR2', 'A', 'B', 'C', 'D']\n",
    "    simul = pd.DataFrame(results, columns=column_order)\n",
    "    simul.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cor_figure.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_figure(filter, truncate=False):\n",
    "    dat_det = pd.read_csv(\"csv/annplant_2spp_det_rare.csv\")\n",
    "    if filter == 'inverted':\n",
    "        dat_det = dat_det.query('Rank == 2 & S1 < 1 & S2 < 1').copy()\n",
    "    elif filter == 'on':\n",
    "        dat_det = dat_det.query('Rank == 2 & S1 >= 1 & S2 >= 1').copy()\n",
    "    else: # 'off'\n",
    "        dat_det = dat_det.query('Rank == 2').copy()\n",
    "    dat_det.reset_index(drop=True, inplace=True)\n",
    "    if truncate:\n",
    "        dat_det = np.trunc(dat_det * 100) / 100.0\n",
    "    dat_det.sort_values(by=['a22', 'a21', 'a12', 'a11', 'r2', 'r1'], inplace=True)\n",
    "    dat_det.to_csv(f\"csv/annplant_2spp_det_rare_filtered_{filter}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figures_det.r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_logistic_regression(dat, analysis_type, print_on=False):\n",
    "    predictors_map = {\n",
    "        'SoS': ['S1', 'FE1', 'cor_sos'],\n",
    "    }    \n",
    "    predictors = predictors_map[analysis_type]\n",
    "    X = sm.add_constant(dat[predictors])\n",
    "    y = dat['Coexist']\n",
    "    model = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "    result = model.fit()\n",
    "    print(result.summary())\n",
    "    coef = result.params\n",
    "    std_err = result.bse\n",
    "    z_scores = result.tvalues\n",
    "    p_values = result.pvalues\n",
    "    intercept = coef[0]\n",
    "    coef = coef[1:]\n",
    "    if print_on: # same analysis in more detail\n",
    "        print(\"\\n\\n--------------------------------------------------------\\n\\n\")\n",
    "        result_table = result.summary2().tables[1]\n",
    "        # Apply maximum precision to coefficient-related statistics\n",
    "        result_table['Coef.'] = result_table['Coef.']\n",
    "        result_table['Std.Err.'] = result_table['Std.Err.']\n",
    "        result_table['z'] = result_table['z'].apply(lambda x: np.format_float_scientific(x, precision=4))\n",
    "        result_table['P>|z|'] = result_table['P>|z|'].apply(lambda x: np.format_float_scientific(x, precision=4))\n",
    "        result_table = result_table.round(4)\n",
    "        print(f\"\\n{analysis_type} Analysis (in more detail):\")\n",
    "        print(result_table)\n",
    "    return intercept, coef, std_err, z_scores, p_values\n",
    "\n",
    "def calculate_proportions(dat, correlation_type):\n",
    "    proportions = {}\n",
    "    for cor_type in [correlation_type]:\n",
    "        proportions[f'positive_coexistence_{cor_type}'] = len(dat[(dat[cor_type] >= 0) & (dat['Coexist'] == 1)])\n",
    "        proportions[f'positive_exclusion_{cor_type}'] = len(dat[(dat[cor_type] >= 0) & (dat['Coexist'] == 0)])\n",
    "        proportions[f'negative_coexistence_{cor_type}'] = len(dat[(dat[cor_type] < 0) & (dat['Coexist'] == 1)])\n",
    "        proportions[f'negative_exclusion_{cor_type}'] = len(dat[(dat[cor_type] < 0) & (dat['Coexist'] == 0)])\n",
    "    return proportions\n",
    "\n",
    "def report_coexistence_analysis(proportions, correlation_type):\n",
    "    positive_key = f'positive_coexistence_{correlation_type}'\n",
    "    negative_key = f'negative_coexistence_{correlation_type}'\n",
    "    neg_confint = proportion_confint(count=proportions[negative_key], nobs=proportions[negative_key] + proportions[f'negative_exclusion_{correlation_type}'], alpha=0.05, method='wilson')\n",
    "    pos_confint = proportion_confint(count=proportions[positive_key], nobs=proportions[positive_key] + proportions[f'positive_exclusion_{correlation_type}'], alpha=0.05, method='wilson')\n",
    "    print(f\"\\nAnalysis on Negative \\u03BD for {correlation_type.upper()}:\")\n",
    "    print(f\"Proportion of coexistence with \\u03BD \\u2265 0: {proportions[positive_key] / (proportions[positive_key] + proportions[f'positive_exclusion_{correlation_type}']):.2g} (95% CI: {pos_confint})\")\n",
    "    print(f\"Proportion of coexistence with \\u03BD < 0: {proportions[negative_key] / (proportions[negative_key] + proportions[f'negative_exclusion_{correlation_type}']):.2g} (95% CI: {neg_confint})\")\n",
    "    \n",
    "def analyze_coexistence_effect(data, print_on):\n",
    "    original_dat = data.copy()\n",
    "    models_results = {}\n",
    "    for correlation_type in ['SoS']:\n",
    "        analysis_type = f'{correlation_type}'\n",
    "        correlation_column = 'cor_sos'\n",
    "        if correlation_column not in data.columns:\n",
    "            continue\n",
    "        print(f\"\\n--- Analysis for {analysis_type} ---\")\n",
    "        intercept, coef, std_err, z_scores, p_values = perform_logistic_regression(data, analysis_type, print_on=print_on)\n",
    "        models_results[analysis_type] = {\n",
    "            'statsmodels': (intercept, coef, std_err, z_scores, p_values),\n",
    "        }\n",
    "        proportions = calculate_proportions(data, correlation_column)\n",
    "        report_coexistence_analysis(proportions, correlation_column)\n",
    "        table_data = {\n",
    "            '\\u03BD \\u2265 0': [proportions[f'positive_coexistence_{correlation_column}'], proportions[f'positive_exclusion_{correlation_column}']],\n",
    "            '\\u03BD < 0': [proportions[f'negative_coexistence_{correlation_column}'], proportions[f'negative_exclusion_{correlation_column}']]\n",
    "        }\n",
    "        table_df = pd.DataFrame(table_data, index=['Coexistence', 'Exclusion'])\n",
    "        print(f\"Coexistence and Exclusion based on \\u03BD for {analysis_type}:\\n\", table_df)\n",
    "        # Proportion of coexistence calculations and confidence intervals\n",
    "        pos_confint = proportion_confint(count=proportions[f'positive_coexistence_{correlation_column}'], nobs=proportions[f'positive_coexistence_{correlation_column}'] + proportions[f'positive_exclusion_{correlation_column}'], alpha=0.05, method='wilson')\n",
    "        neg_confint = proportion_confint(count=proportions[f'negative_coexistence_{correlation_column}'], nobs=proportions[f'negative_coexistence_{correlation_column}'] + proportions[f'negative_exclusion_{correlation_column}'], alpha=0.05, method='wilson')\n",
    "        # Decision making based on confidence intervals\n",
    "        if neg_confint[1] >= pos_confint[0] and neg_confint[0] <= pos_confint[1]:  # Overlap\n",
    "            print(f\"The confidence intervals overlap for {analysis_type}, indicating they are statistically the same, not supporting the authors' results.\")\n",
    "        elif neg_confint[1] > pos_confint[0]:  # Negative larger than positive\n",
    "            print(f\"Higher coexistence observed with \\u03BD < 0 for {analysis_type}, supporting the authors' results.\")\n",
    "        else:  # Negative smaller than positive\n",
    "            print(f\"Higher coexistence observed with \\u03BD \\u2265 0 for {analysis_type}, not supporting the authors' results.\")\n",
    "    return models_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_abcd(filtered_data):\n",
    "    # Initialize counters\n",
    "    count_A_0, count_A_1 = 0, 0\n",
    "    count_B_0, count_B_1 = 0, 0\n",
    "    count_C_0, count_C_1 = 0, 0\n",
    "    count_D_0, count_D_1 = 0, 0\n",
    "    # Loop over the filtered dataset and apply the conditions for A, B, C, D\n",
    "    for index, row in filtered_data.iterrows():\n",
    "        P = (row['r1'] - 1) / row['a12']\n",
    "        E2 = (row['r2'] - 1) / row['a22']\n",
    "        Q = (row['r2'] - 1) / row['a21']\n",
    "        E1 = (row['r1'] - 1) / row['a11']\n",
    "        if P != E2 and Q != E1:\n",
    "            if P > E2 and E1 > Q:  # Case A\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_A_0 += 1\n",
    "                else:\n",
    "                    count_A_1 += 1\n",
    "            elif E2 > P and Q > E1:  # Case B\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_B_0 += 1\n",
    "                else:\n",
    "                    count_B_1 += 1\n",
    "            elif P > E2 and Q > E1:  # Case C\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_C_0 += 1\n",
    "                else:\n",
    "                    count_C_1 += 1\n",
    "            elif E2 > P and E1 > Q:  # Case D\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_D_0 += 1\n",
    "                else:\n",
    "                    count_D_1 += 1\n",
    "        elif P == E2 and Q == E1:\n",
    "            if row['Coexist'] == 0:\n",
    "                count_C_0 += 1\n",
    "            else:\n",
    "                count_C_1 += 1\n",
    "        elif P == E2:  # If P equals E2, only look at the relationship between Q and E1\n",
    "            if Q > E1:\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_B_0 += 1\n",
    "                else:\n",
    "                    count_B_1 += 1\n",
    "            elif E1 > Q:\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_A_0 += 1\n",
    "                else:\n",
    "                    count_A_1 += 1\n",
    "        elif Q == E1:  # If Q equals E1, only look at the relationship between P and E2\n",
    "            if P > E2:\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_A_0 += 1\n",
    "                else:\n",
    "                    count_A_1 += 1\n",
    "            elif E2 > P:\n",
    "                if row['Coexist'] == 0:\n",
    "                    count_B_0 += 1\n",
    "                else:\n",
    "                    count_B_1 += 1\n",
    "    # Calculate totals for A, B, C, D\n",
    "    count_A_total = count_A_0 + count_A_1\n",
    "    count_B_total = count_B_0 + count_B_1\n",
    "    count_C_total = count_C_0 + count_C_1\n",
    "    count_D_total = count_D_0 + count_D_1\n",
    "    total_count = len(filtered_data)\n",
    "    # Calculate proportions\n",
    "    prop_A = count_A_total / total_count if total_count != 0 else 0\n",
    "    prop_B = count_B_total / total_count if total_count != 0 else 0\n",
    "    prop_C = count_C_total / total_count if total_count != 0 else 0\n",
    "    prop_D = count_D_total / total_count if total_count != 0 else 0\n",
    "    # Print results\n",
    "    print(f\"A\\nCoexist==0: {count_A_0}\\nCoexist==1: {count_A_1}\\nTotal: {count_A_total}\\nProportion: {prop_A:.2g}\")\n",
    "    print(f\"\\nB\\nCoexist==0: {count_B_0}\\nCoexist==1: {count_B_1}\\nTotal: {count_B_total}\\nProportion: {prop_B:.2g}\")\n",
    "    print(f\"\\nC\\nCoexist==0: {count_C_0}\\nCoexist==1: {count_C_1}\\nTotal: {count_C_total}\\nProportion: {prop_C:.2g}\")\n",
    "    print(f\"\\nD\\nCoexist==0: {count_D_0}\\nCoexist==1: {count_D_1}\\nTotal: {count_D_total}\\nProportion: {prop_D:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phase_plane():\n",
    "    # Parameters for each scenario\n",
    "    scenarios = {\n",
    "        \"A: $E_1 > Q$ and $P > E_2$\": {'r1': 18, 'r2': 16, 'a11': 0.5, 'a12': 1, 'a21': 1, 'a22': 1},\n",
    "        \"B: $Q > E_1$ and $E_2 > P$\": {'r1': 20, 'r2': 15, 'a11': 2, 'a12': 1, 'a21': 1, 'a22': 0.5},\n",
    "        \"C: $Q > E_1$ and $P > E_2$\":  {'r1': 20, 'r2': 15, 'a11': 3, 'a12': 0.5, 'a21': 1, 'a22': 0.5},\n",
    "        \"D: $E_1 > Q$ and $E_2 > P$\":  {'r1': 16, 'r2': 18, 'a11': 0.3, 'a12': 1, 'a21': 1, 'a22': 0.5},\n",
    "    }    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()    \n",
    "    for i, (title, params) in enumerate(scenarios.items()):\n",
    "        ax = axes[i]\n",
    "        # Unpack parameters\n",
    "        r1 = params['r1']\n",
    "        r2 = params['r2']\n",
    "        a11 = params['a11']\n",
    "        a12 = params['a12']\n",
    "        a21 = params['a21']\n",
    "        a22 = params['a22']\n",
    "        # Equilibrium points\n",
    "        E1 = [(r1 - 1) / a11, 0]\n",
    "        Q = [(r2 - 1) / a21, 0]\n",
    "        E2 = [0, (r2 - 1) / a22]\n",
    "        P = [0, (r1 - 1) / a12]\n",
    "        E0 = [0, 0]\n",
    "        # Calculate the intersection point of lines (E1, Q) and (E2, P)\n",
    "        a1 = (P[1] - E1[1]) / (P[0] - E1[0]) if P[0] != E1[0] else float('inf')\n",
    "        b1 = E1[1] - a1 * E1[0]\n",
    "        a2 = (E2[1] - Q[1]) / (E2[0] - Q[0]) if E2[0] != Q[0] else float('inf')\n",
    "        b2 = Q[1] - a2 * Q[0]\n",
    "        if a1 != a2:  # Ensure lines are not parallel\n",
    "            E3_x = (b2 - b1) / (a1 - a2) if a1 != float('inf') and a2 != float('inf') else 0\n",
    "            E3_y = a1 * E3_x + b1 if a1 != float('inf') else a2 * E3_x + b2\n",
    "            E3 = [E3_x, E3_y]\n",
    "        else:\n",
    "            E3 = None\n",
    "        # Extend axis limits by 10%\n",
    "        max_N1 = max(E1[0], Q[0])\n",
    "        max_N2 = max(E2[1], P[1])\n",
    "        N1 = np.linspace(0, max_N1, 30)\n",
    "        N2 = np.linspace(0, max_N2, 30)\n",
    "        N1, N2 = np.meshgrid(N1, N2)\n",
    "        # Compute the discrete system\n",
    "        N1_next = r1 * N1 / (1 + a11 * N1 + a12 * N2)\n",
    "        N2_next = r2 * N2 / (1 + a22 * N2 + a21 * N1)\n",
    "        # Plot vector field\n",
    "        ax.quiver(N1, N2, N1_next - N1, N2_next - N2, angles='xy', scale_units='xy', scale=15, color='gray', alpha=1)\n",
    "        # Plot equilibrium points\n",
    "        ax.plot(E0[0], E0[1], 'ko', label='E0', markersize=8)\n",
    "        ax.plot(E1[0], E1[1], 'bo', label='E1', markersize=8)\n",
    "        ax.plot(Q[0], Q[1], 'ro', label='Q', markersize=8)\n",
    "        ax.plot(E2[0], E2[1], 'ro', label='E2', markersize=8)\n",
    "        ax.plot(P[0], P[1], 'bo', label='P', markersize=8)\n",
    "        # Draw lines between points\n",
    "        ax.plot([E1[0], P[0]], [E1[1], P[1]], 'b-', lw=2)  # Line between P and E1 (blue)\n",
    "        ax.plot([Q[0], E2[0]], [Q[1], E2[1]], 'r-', lw=2)  # Line between Q and E2 (red)\n",
    "        # Plot intersection point E3 if it exists within the plot limits and above the lines\n",
    "        if E3 is not None and (0 <= E3[0] <= 1.1 * max_N1) and (0 <= E3[1] <= 1.1 * max_N2):\n",
    "            ax.plot(E3[0], E3[1], 'go', label=r'$E_3$', markersize=8)\n",
    "            # Annotate E3 near the point\n",
    "            ax.annotate(f'$E_3$', xy=(E3[0], E3[1]), xytext=(E3[0] + 0.3, E3[1] + 0.3), fontsize=18, color='green')\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel(r'$N_1$', fontsize=18)\n",
    "        ax.set_ylabel(r'$N_2$', fontsize=18)\n",
    "        # Move title to the left\n",
    "        ax.set_title(title, fontsize=18, loc='left')\n",
    "        # Set xticks and yticks with labels for E1, E2, P, Q\n",
    "        ax.set_xticks([0, E1[0], Q[0]])\n",
    "        ax.set_xticklabels([r'$E_0$', r'$E_1$', r'$Q$'])\n",
    "        ax.set_yticks([0, E2[1], P[1]])\n",
    "        ax.set_yticklabels([r'$E_0$', r'$E_2$', r'$P$'])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    # Adjust layout and save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('img/phase_plane.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def plot_pgr_figures(filter_option, extinc_crit_1=False):\n",
    "    plt.rcParams.update({\n",
    "        'axes.titlesize': 14,\n",
    "        'axes.labelsize': 18,\n",
    "        'xtick.labelsize': 16,\n",
    "        'ytick.labelsize': 16,\n",
    "        'legend.fontsize': 12,\n",
    "        'font.size': 18,\n",
    "        'lines.linewidth': 1.5\n",
    "    })\n",
    "    # Prepare outputs\n",
    "    os.makedirs('png', exist_ok=True)\n",
    "    summary_path = f\"csv/pgr_analysis_summary_{filter_option}.csv\"\n",
    "    # Initialise CSV with proper columns\n",
    "    cols = [\n",
    "        'k', 'r1', 'r2', 'a11', 'a12', 'a21', 'a22',\n",
    "        'cor_sos', 'nu_sign', 'coexist',\n",
    "        'left_PGR1_dominant', 'right_PGR1_dominant', 'curve_cross'\n",
    "    ]\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(','.join(cols) + '\\n')\n",
    "    try:\n",
    "        # Load data\n",
    "        df = pd.read_csv(f\"csv/annplant_2spp_det_rare_filtered_{filter_option}.csv\")\n",
    "        df['cor_sos'] = pd.to_numeric(df['cor_sos'], errors='coerce')\n",
    "        df['Coexist'] = pd.to_numeric(df['Coexist'], errors='coerce').fillna(0).astype(int)\n",
    "        # Categorise nu values\n",
    "        conditions = [\n",
    "            df['cor_sos'] < -0.001,\n",
    "            df['cor_sos'].abs() <= 0.001,\n",
    "            df['cor_sos'] > 0.001\n",
    "        ]\n",
    "        df['nu_sign'] = np.select(conditions, ['negative', 'zero', 'positive'], default='invalid')\n",
    "        # Define k values to vary\n",
    "        k_values = [0.25, 0.5, 0.75]\n",
    "        # Process each parameter set and k value\n",
    "        for idx, row in df.iterrows():\n",
    "            if row['nu_sign'] == 'invalid':\n",
    "                continue\n",
    "            for k in k_values:\n",
    "                # 1. Calculate edge dominance with N2 = N1/k\n",
    "                # Left edge (N1 ~ 0)\n",
    "                N1_left = 1e-9\n",
    "                N2_left = N1_left / k\n",
    "                pgr1_left, pgr2_left = getPCG(\n",
    "                    row['r1'], row['r2'],\n",
    "                    row['a11'], row['a12'],\n",
    "                    row['a21'], row['a22'],\n",
    "                    N1_left, N2_left\n",
    "                )\n",
    "                left_PGR1_dominant = int(pgr1_left > pgr2_left) if not (\n",
    "                    np.isnan(pgr1_left) or np.isnan(pgr2_left)) else np.nan\n",
    "                # Right edge (N1 ~ 1)\n",
    "                N1_right = 1.0 - 1e-9\n",
    "                N2_right = N1_right / k\n",
    "                pgr1_right, pgr2_right = getPCG(\n",
    "                    row['r1'], row['r2'],\n",
    "                    row['a11'], row['a12'],\n",
    "                    row['a21'], row['a22'],\n",
    "                    N1_right, N2_right\n",
    "                )\n",
    "                right_PGR1_dominant = int(pgr1_right > pgr2_right) if not (\n",
    "                    np.isnan(pgr1_right) or np.isnan(pgr2_right)) else np.nan\n",
    "                # 2. Generate plot data with N2 = N1/k\n",
    "                freqs = np.linspace(0, 1, 100)\n",
    "                pgr1, pgr2 = [], []\n",
    "                for f in freqs:\n",
    "                    N1 = max(f, 1e-9)\n",
    "                    N2 = N1 / k\n",
    "                    pgri, pgrj = getPCG(\n",
    "                        row['r1'], row['r2'],\n",
    "                        row['a11'], row['a12'],\n",
    "                        row['a21'], row['a22'],\n",
    "                        N1, N2\n",
    "                    )\n",
    "                    pgr1.append(pgri if not np.isnan(pgri) else np.nan)\n",
    "                    pgr2.append(pgrj if not np.isnan(pgrj) else np.nan)\n",
    "                # 3. Save results\n",
    "                csv_line = (\n",
    "                    f\"{k},{row['r1']},{row['r2']},\"\n",
    "                    f\"{row['a11']},{row['a12']},\"\n",
    "                    f\"{row['a21']},{row['a22']},\"\n",
    "                    f\"{row['cor_sos']},{row['nu_sign']},\"\n",
    "                    f\"{row['Coexist']},{left_PGR1_dominant},{right_PGR1_dominant}\\n\"\n",
    "                )\n",
    "                with open(summary_path, 'a') as f:\n",
    "                    f.write(csv_line)\n",
    "                # 4. Plotting\n",
    "                fig = plt.figure(figsize=(10,6))\n",
    "                try:\n",
    "                    ax = fig.add_subplot(111)\n",
    "                    ax.plot(freqs, pgr1, '-', color='blue', label='N1')\n",
    "                    ax.plot(freqs, pgr2, '--', color='orange', label='N2')\n",
    "                    ax.set_xlabel(\"Frequency of N1\")\n",
    "                    ax.set_ylabel(\"log(PGR)\")\n",
    "                    ax.set_title(\n",
    "                        f\"\\u03BD={row['cor_sos']:.2g}, \"\n",
    "                        f\"Coexist={row['Coexist']}, k={k},\\n\"\n",
    "                        f\"r1={row['r1']} a11={row['a11']} a12={row['a12']}\\n\"\n",
    "                        f\"r2={row['r2']} a21={row['a21']} a22={row['a22']}\"\n",
    "                    )\n",
    "                    ax.axhline(0, color='black', linestyle=':', linewidth=0.8)\n",
    "                    ax.legend()\n",
    "                    fname = (\n",
    "                        f\"png/r1_{row['r1']}_r2_{row['r2']}_\"\n",
    "                        f\"a11_{row['a11']}_a12_{row['a12']}_\"\n",
    "                        f\"a21_{row['a21']}_a22_{row['a22']}_pgr_k_{k}.png\"\n",
    "                    )\n",
    "                    fig.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "                finally:\n",
    "                    plt.close(fig)\n",
    "                    ax.remove()\n",
    "                    del ax, fig, freqs, pgr1, pgr2\n",
    "                    gc.collect()\n",
    "        # Generate final reports\n",
    "        print(\"\\n=== Final Analysis Report ===\")\n",
    "        summary_df = pd.read_csv(summary_path)\n",
    "        print(\"\\nEdge Dominance Statistics:\")\n",
    "        for group in ['negative', 'zero', 'positive']:\n",
    "            group_df = summary_df[summary_df.nu_sign == group]\n",
    "            if group_df.empty:\n",
    "                continue\n",
    "            print(f\"\\n\\u03BD {group.capitalize()} (N={len(group_df)}):\")\n",
    "            print(f\"Left edge N1 wins:  {group_df.left_PGR1_dominant.mean():.1%}\")\n",
    "            print(f\"Right edge N1 wins: {group_df.right_PGR1_dominant.mean():.1%}\")\n",
    "        print(\"\\nCoexistence Distribution:\")\n",
    "        print(pd.crosstab(\n",
    "            index=summary_df.nu_sign,\n",
    "            columns=summary_df.coexist,\n",
    "            margins=True,\n",
    "            margins_name=\"Total\"\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pgr_figures(filter_option, save_fig=False, extinc_crit_1=False):\n",
    "    plt.rcParams.update({\n",
    "        'axes.titlesize': 14,\n",
    "        'axes.labelsize': 18,\n",
    "        'xtick.labelsize': 16,\n",
    "        'ytick.labelsize': 16,\n",
    "        'legend.fontsize': 12,\n",
    "        'font.size': 18,\n",
    "        'lines.linewidth': 1.5\n",
    "    })\n",
    "    summary_path = f\"csv/pgr_analysis_summary_{filter_option}.csv\"\n",
    "    cols = [\n",
    "        'k', 'r1', 'r2', 'a11', 'a12', 'a21', 'a22',\n",
    "        'cor_sos', 'nu_sign', 'coexist',\n",
    "        'left_PGR1_dominant', 'right_PGR1_dominant', 'curve_cross'\n",
    "    ]\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(','.join(cols) + '\\n')\n",
    "    try:\n",
    "        df = pd.read_csv(f\"csv/annplant_2spp_det_rare_filtered_{filter_option}.csv\")\n",
    "        df['cor_sos'] = pd.to_numeric(df['cor_sos'], errors='coerce')\n",
    "        df['Coexist'] = pd.to_numeric(df['Coexist'], errors='coerce').fillna(0).astype(int)\n",
    "        conditions = [\n",
    "            df['cor_sos'] < -0.001,\n",
    "            df['cor_sos'].abs() <= 0.001,\n",
    "            df['cor_sos'] > 0.001\n",
    "        ]\n",
    "        df['nu_sign'] = np.select(conditions, ['negative', 'zero', 'positive'], default='invalid')\n",
    "        k_values = [0.25] # , 0.5, 0.75\n",
    "        for idx, row in df.iterrows():\n",
    "            if row['nu_sign'] == 'invalid':\n",
    "                continue\n",
    "            for k in k_values:\n",
    "                # Edge dominance\n",
    "                N1_left = 1e-9\n",
    "                N2_left = 1 - N1_left\n",
    "                pgr1_left, pgr2_left = getPCG(\n",
    "                    row['r1'], row['r2'],\n",
    "                    row['a11'], row['a12'],\n",
    "                    row['a21'], row['a22'],\n",
    "                    N1_left, N2_left\n",
    "                )\n",
    "                left_PGR1_dominant = int(pgr1_left > pgr2_left) if not (\n",
    "                    np.isnan(pgr1_left) or np.isnan(pgr2_left)) else np.nan\n",
    "                N1_right = 1.0 - 1e-9\n",
    "                N2_right = 1 - N1_right\n",
    "                pgr1_right, pgr2_right = getPCG(\n",
    "                    row['r1'], row['r2'],\n",
    "                    row['a11'], row['a12'],\n",
    "                    row['a21'], row['a22'],\n",
    "                    N1_right, N2_right\n",
    "                )\n",
    "                right_PGR1_dominant = int(pgr1_right > pgr2_right) if not (\n",
    "                    np.isnan(pgr1_right) or np.isnan(pgr2_right)) else np.nan\n",
    "                curve_cross = int(left_PGR1_dominant != right_PGR1_dominant)\n",
    "                # Frequency sweep (N1 + N2 = 1)\n",
    "                freqs = np.linspace(0, 1, 100)\n",
    "                pgr1, pgr2 = [], []\n",
    "                for f in freqs:\n",
    "                    if f == 0 or f == 1:\n",
    "                        f = max(min(f, 1 - 1e-9), 1e-9)\n",
    "                    N1 = f\n",
    "                    N2 = 1 - f\n",
    "                    pgri, pgrj = getPCG(\n",
    "                        row['r1'], row['r2'],\n",
    "                        row['a11'], row['a12'],\n",
    "                        row['a21'], row['a22'],\n",
    "                        N1, N2\n",
    "                    )\n",
    "                    pgr1.append(pgri if not np.isnan(pgri) else np.nan)\n",
    "                    pgr2.append(pgrj if not np.isnan(pgrj) else np.nan)\n",
    "                csv_line = (\n",
    "                    f\"{k},{row['r1']},{row['r2']},\"\n",
    "                    f\"{row['a11']},{row['a12']},\"\n",
    "                    f\"{row['a21']},{row['a22']},\"\n",
    "                    f\"{row['cor_sos']},{row['nu_sign']},\"\n",
    "                    f\"{row['Coexist']},{left_PGR1_dominant},{right_PGR1_dominant},{curve_cross}\\n\"\n",
    "                )\n",
    "                with open(summary_path, 'a') as f:\n",
    "                    f.write(csv_line)\n",
    "                fig = plt.figure(figsize=(10,6))\n",
    "                try:\n",
    "                    ax = fig.add_subplot(111)\n",
    "                    ax.plot(freqs, pgr1, '-', color='blue', label='N1')\n",
    "                    ax.plot(freqs, pgr2, '--', color='orange', label='N2')\n",
    "                    ax.set_xlabel(\"Frequency of N1\")\n",
    "                    ax.set_ylabel(\"log(PGR)\")\n",
    "                    ax.set_title(\n",
    "                        f\"\\u03BD={row['cor_sos']:.2g}, \"\n",
    "                        f\"Coexist={row['Coexist']},\\n\" # , k={k}\n",
    "                        f\"r1={row['r1']} a11={row['a11']} a12={row['a12']}\\n\"\n",
    "                        f\"r2={row['r2']} a21={row['a21']} a22={row['a22']}\"\n",
    "                    )\n",
    "                    ax.axhline(0, color='black', linestyle=':', linewidth=0.8)\n",
    "                    ax.legend()\n",
    "                    fname = (\n",
    "                        f\"png/r1_{row['r1']}_r2_{row['r2']}_\"\n",
    "                        f\"a11_{row['a11']}_a12_{row['a12']}_\"\n",
    "                        f\"a21_{row['a21']}_a22_{row['a22']}.png\" # _pgr_k_{k}_freq\n",
    "                    )\n",
    "                    if save_fig:\n",
    "                        os.makedirs('png', exist_ok=True)\n",
    "                        fig.savefig(fname, dpi=150, bbox_inches='tight')\n",
    "                finally:\n",
    "                    plt.close(fig)\n",
    "                    ax.remove()\n",
    "                    del ax, fig, freqs, pgr1, pgr2\n",
    "                    gc.collect()\n",
    "        print(\"\\n=== Analysis Report ===\")\n",
    "        summary_df = pd.read_csv(summary_path)\n",
    "        print(\"\\nEdge Dominance Statistics:\")\n",
    "        for sign in ['negative', 'zero', 'positive']:\n",
    "            sign_df = summary_df[summary_df.nu_sign == sign]\n",
    "            if sign_df.empty:\n",
    "                continue\n",
    "            print(f\"\\n\\u03BD {sign.capitalize()} (N={len(sign_df)}):\")\n",
    "            print(f\"Left edge N1 wins:  {sign_df.left_PGR1_dominant.mean():.1%}\")\n",
    "            print(f\"Right edge N1 wins: {sign_df.right_PGR1_dominant.mean():.1%}\")\n",
    "        print(\"\\nCoexistence Distribution:\")\n",
    "        print(pd.crosstab(\n",
    "            index=summary_df.nu_sign,\n",
    "            columns=summary_df.coexist,\n",
    "            margins=True,\n",
    "            margins_name=\"Total\"\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hypotheses_descriptive(filter_option, summary_path):\n",
    "    df = pd.read_csv(summary_path)\n",
    "    # Define hypothesis flags\n",
    "    df['H1'] = (df['nu_sign'] == 'positive').astype(int)\n",
    "    df['H2'] = df['curve_cross']\n",
    "    df['H3'] = df['left_PGR1_dominant']\n",
    "    # df['H4'] = df['k'].apply(lambda x: 1 if x < 1 else 0)  # k < 1\n",
    "    results = []\n",
    "    hypotheses = ['H1', 'H2', 'H3']#, 'H4']\n",
    "    # Test all combinations\n",
    "    for r in range(1, 5):\n",
    "        for combo in combinations(hypotheses, r):\n",
    "            combo_name = \"+\".join(combo)\n",
    "            mask = df[list(combo)].all(axis=1)\n",
    "            n_cases = mask.sum()\n",
    "            if n_cases == 0:\n",
    "                continue\n",
    "            coexist_proportion = df.loc[mask, 'coexist'].mean()\n",
    "            results.append({\n",
    "                'Hypothesis': combo_name,\n",
    "                'Coexist Proportion': f\"{coexist_proportion:.1%}\",\n",
    "                'N Cases': n_cases\n",
    "            })\n",
    "    results_df = pd.DataFrame(results).sort_values('Coexist Proportion', ascending=False)\n",
    "    print(\"\\n=== Descriptive Analysis (Truth Table) ===\")\n",
    "    print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def analyze_hypotheses_bayesian(filter_option, summary_path):\n",
    "    df = pd.read_csv(summary_path)\n",
    "    # Encode predictors\n",
    "    df['nu_positive'] = (df['nu_sign'] == 'positive').astype(int)\n",
    "    df['nu_negative'] = (df['nu_sign'] == 'negative').astype(int)\n",
    "    df['k_cat'] = pd.Categorical(df['k']).codes\n",
    "    with pm.Model() as model:\n",
    "        # Priors (weakly informative)\n",
    "        alpha = pm.Normal('alpha', mu=0, sigma=1)\n",
    "        beta = pm.Normal('beta', mu=0, sigma=1, shape=4)\n",
    "        # Linear predictor\n",
    "        mu = alpha + (\n",
    "            beta[0] * df['nu_positive'] +\n",
    "            beta[1] * df['nu_negative'] +\n",
    "            beta[2] * df['curve_cross'] +\n",
    "            beta[3] * df['k_cat']\n",
    "        )\n",
    "        p = pm.math.sigmoid(mu)\n",
    "        # Likelihood\n",
    "        likelihood = pm.Bernoulli('likelihood', p=p, observed=df['coexist'])\n",
    "        # Inference\n",
    "        trace = pm.sample(2000, tune=1000, target_accept=0.95)\n",
    "    print(pm.summary(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def analyze_hypotheses_rf(filter_option, summary_path, seed=1234):\n",
    "    df = pd.read_csv(summary_path)\n",
    "    # Encode features\n",
    "    X = pd.get_dummies(df[['nu_sign', 'curve_cross', 'left_PGR1_dominant', 'k']])\n",
    "    y = df['coexist']\n",
    "    # Train RF model\n",
    "    model = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=seed)\n",
    "    model.fit(X, y)\n",
    "    # Permutation importance\n",
    "    result = permutation_importance(model, X, y, n_repeats=10, random_state=seed, n_jobs=-1)\n",
    "    # Feature importance table\n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': result.importances_mean,\n",
    "        'Std': result.importances_std\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    print(\"\\n=== Random Forest Feature Importance ===\")\n",
    "    print(importance.to_string(index=False))\n",
    "    # SHAP directionality analysis\n",
    "    print(\"\\n=== SHAP Directional Impacts ===\")\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values[1], X, show=False)\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('shap', exist_ok=True)\n",
    "    plt.savefig(f'shap/shap_{filter_option}.png', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hypotheses_rf(filter_option, summary_path, seed=1234):\n",
    "    df = pd.read_csv(summary_path)\n",
    "    nu_map = {'negative': 0, 'zero': 1, 'positive': 2}\n",
    "    df['nu_sign'] = df['nu_sign'].map(nu_map)\n",
    "    required_features = ['nu_sign', 'curve_cross', 'left_PGR1_dominant', 'k'] # Hypotheses\n",
    "    X = df[required_features].copy()\n",
    "    y = df['coexist'].values\n",
    "    unique_classes = np.unique(y)\n",
    "    if unique_classes.size != 2:\n",
    "        raise ValueError(f\"Coexist column must be binary (0/1). Found: {unique_classes}\")\n",
    "    model = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=seed, n_jobs=-1, class_weight='balanced') # Fit Random Forest\n",
    "    model.fit(X, y)\n",
    "    perm_imp = permutation_importance(model, X, y, n_repeats=10, random_state=seed, n_jobs=-1) # Permutation importance\n",
    "    # SHAP analysis\n",
    "    shap_imp = pd.Series(np.nan, index=X.columns)\n",
    "    shap_vals = None\n",
    "    try:\n",
    "        explainer = shap.Explainer(model, X, feature_perturbation=\"interventional\")\n",
    "        sv = explainer(X)\n",
    "        vals = sv.values\n",
    "        if vals.ndim == 3:\n",
    "            shap_vals = vals[:, :, 1]\n",
    "        elif vals.ndim == 2:\n",
    "            shap_vals = vals\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported SHAP values ndim: {vals.ndim}\")\n",
    "        if shap_vals.shape == (X.shape[1], X.shape[0]):\n",
    "            shap_vals = shap_vals.T\n",
    "        if shap_vals.shape != (X.shape[0], X.shape[1]):\n",
    "            raise ValueError(f\"SHAP/Feature dimension mismatch after transpose check: {shap_vals.shape} vs {X.shape}\")\n",
    "        abs_imp = np.abs(shap_vals).mean(axis=0)\n",
    "        total = abs_imp.sum()\n",
    "        if total > 0:\n",
    "            shap_imp = pd.Series((abs_imp / total).round(3), index=X.columns)\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP Calculation Error: {e}\")\n",
    "    # Build and print the importance table\n",
    "    imp_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Permutation Importance': perm_imp.importances_mean.round(3),\n",
    "        # 'Permutation Std': perm_imp.importances_std.round(3),\n",
    "        'SHAP Impact (%)': (shap_imp * 100).round(1)\n",
    "    }).sort_values('SHAP Impact (%)', ascending=False)\n",
    "    print(\"\\n=== Feature Importance ===\")\n",
    "    print(imp_df.to_string(index=False, float_format=\"%.2g\"))\n",
    "    if shap_vals is not None:\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            shap.summary_plot(\n",
    "                shap_vals, X.values,\n",
    "                feature_names=X.columns,\n",
    "                plot_type='dot', show=False\n",
    "            )\n",
    "            plt.tight_layout()\n",
    "            os.makedirs('shap', exist_ok=True)\n",
    "            plt.savefig(f'shap/shap_{filter_option}.png', dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP Visualization Error: {e}\")\n",
    "    else:\n",
    "        print(\"Skipping SHAP visualization due to missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hypotheses(filter_option, summary_path):\n",
    "    \"\"\"Hypothesis testing\"\"\"\n",
    "    # Print hypotheses first\n",
    "    print(\"\\n\" + \"=\"*140)\n",
    "    print(\"Hypothesis Definitions:\")\n",
    "    print(\"\\n(H1) Sign of \\u03BD (cor_sos)\")\n",
    "    print(\"\\n(H2) Curve crossing (left/right edge dominance differs)\")\n",
    "    print(\"\\n(H3) PGR1 at left edge is higher\")\n",
    "    # print(\"\\n(H4) Value of k parameter\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv(summary_path)\n",
    "    df['curve_cross'] = (df['left_PGR1_dominant'] != df['right_PGR1_dominant']).astype(int)\n",
    "    print(\"\\n--- Hypothesis Analysis Results ---\")\n",
    "    # # 1. Descriptive Analysis\n",
    "    # analyze_hypotheses_descriptive(filter_option, summary_path)\n",
    "    # 2. Random Forest Analysis\n",
    "    analyze_hypotheses_rf(filter_option, summary_path)\n",
    "    # # 3. Bayesian Analysis\n",
    "    # print(\"\\n--- Bayesian Regression Results ---\")\n",
    "    # try:\n",
    "    #     analyze_hypotheses_bayesian(filter_option, summary_path)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Bayesian analysis failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_coexistence_deterministic(filter_option):\n",
    "    summary_path = f\"csv/pgr_analysis_summary_{filter_option}.csv\"\n",
    "    if not os.path.exists(summary_path):\n",
    "        print(f\"\\n-----------\\nGenerating analysis data for {filter_option}...\")\n",
    "        plot_pgr_figures(filter_option, save_fig=False, extinc_crit_1=False)\n",
    "    # Load data\n",
    "    try:\n",
    "        df = pd.read_csv(summary_path)\n",
    "        if df.empty:\n",
    "            raise ValueError(\"Empty summary file - regenerate manually\")\n",
    "        # Clean and prepare data\n",
    "        df = df[df['nu_sign'].isin(['negative', 'zero', 'positive'])]\n",
    "        df['curve_cross'] = (df['left_PGR1_dominant'] != df['right_PGR1_dominant']).astype(int)\n",
    "        print(f\"\\nLoaded {len(df)} valid records\")\n",
    "        # Run analysis\n",
    "        analyze_hypotheses(filter_option, summary_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Analysis failed: {str(e)}\")\n",
    "        return\n",
    "    # Common settings\n",
    "    nu_order = ['negative', 'zero', 'positive']\n",
    "    cross_labels = ['No Cross', 'Cross']\n",
    "    nu_symbols = {'negative': '\\u03BD<0', 'zero': '\\u03BD\\u2248 0', 'positive': '\\u03BD>0'}\n",
    "    plt.rcParams.update({\n",
    "        'axes.titlesize': 14,\n",
    "        'axes.labelsize': 12,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'legend.fontsize': 10,\n",
    "        'font.size': 12,\n",
    "        'lines.linewidth': 1.5\n",
    "    })\n",
    "    # 1. Coexistence vs Curve Crossing\n",
    "    s1 = df.groupby('curve_cross')['coexist'].agg(['sum', 'count'])\n",
    "    s1['p_co'] = s1['sum'] / s1['count']\n",
    "    s1['p_no'] = 1 - s1['p_co']\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    x = np.arange(len(s1))\n",
    "    ax.bar(x, s1['p_co'], color='blue', label='Coexist')\n",
    "    ax.bar(x, s1['p_no'], bottom=s1['p_co'], color='red', label='Non-coexist')\n",
    "    # Annotate each segment\n",
    "    for i in x:\n",
    "        total = s1.iloc[i]['count']\n",
    "        p_co = s1.iloc[i]['p_co']\n",
    "        p_no = s1.iloc[i]['p_no']\n",
    "        sum_co = s1.iloc[i]['sum']\n",
    "        sum_no = total - sum_co\n",
    "        # Coexist segment\n",
    "        ax.text(i, p_co/2, f\"{p_co:.1%}\\n({sum_co})\",  # :.0g\n",
    "                ha='center', va='center', color='white', fontsize=8)\n",
    "        # Non-coexist segment\n",
    "        ax.text(i, p_co + p_no/2, f\"{p_no:.1%}\\n({sum_no})\",  # :.0g\n",
    "                ha='center', va='center', color='white', fontsize=8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(cross_labels)\n",
    "    ax.set_ylabel(\"Percentage\")\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda v, _: f\"{v:.0%}\"))\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'img/hypothesis_1_{filter_option}.png', dpi=300)\n",
    "    plt.close()\n",
    "    # 2. Coexistence vs nu sign\n",
    "    s2 = df.groupby('nu_sign')['coexist'].agg(['sum', 'count']).reindex(nu_order)\n",
    "    s2['p_co'] = s2['sum'] / s2['count']\n",
    "    s2['p_no'] = 1 - s2['p_co']\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    x = np.arange(len(s2))\n",
    "    ax.bar(x, s2['p_co'], color='blue', label='Coexist')\n",
    "    ax.bar(x, s2['p_no'], bottom=s2['p_co'], color='red', label='Non-coexist')\n",
    "    # Annotate each segment\n",
    "    for i in x:\n",
    "        total = s2.iloc[i]['count']\n",
    "        p_co = s2.iloc[i]['p_co']\n",
    "        p_no = s2.iloc[i]['p_no']\n",
    "        sum_co = s2.iloc[i]['sum']\n",
    "        sum_no = total - sum_co\n",
    "        ax.text(i, p_co/2, f\"{p_co:.1%}\\n({sum_co})\",  # :.0g\n",
    "                ha='center', va='center', color='white', fontsize=8)\n",
    "        ax.text(i, p_co + p_no/2, f\"{p_no:.1%}\\n({sum_no})\",  # :.0g\n",
    "                ha='center', va='center', color='white', fontsize=8)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([nu_symbols[nu] for nu in nu_order])\n",
    "    ax.set_ylabel(\"Percentage\")\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda v, _: f\"{v:.0%}\"))\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'img/hypothesis_2_{filter_option}.png', dpi=300)\n",
    "    plt.close()\n",
    "    # 3. Coexistence vs Curve Crossing by nu sign\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "    for ax, nu in zip(axes, nu_order):\n",
    "        sub = df[df['nu_sign'] == nu]\n",
    "        s3 = sub.groupby('curve_cross')['coexist'].agg(['sum', 'count'])\n",
    "        if s3.empty:\n",
    "            continue\n",
    "        s3['p_co'] = s3['sum'] / s3['count']\n",
    "        s3['p_no'] = 1 - s3['p_co']\n",
    "        x_sub = np.arange(len(s3))\n",
    "        ax.bar(x_sub, s3['p_co'], color='blue')\n",
    "        ax.bar(x_sub, s3['p_no'], bottom=s3['p_co'], color='red')\n",
    "        # Annotate each segment\n",
    "        for j in x_sub:\n",
    "            total = s3.iloc[j]['count']\n",
    "            p_co = s3.iloc[j]['p_co']\n",
    "            p_no = s3.iloc[j]['p_no']\n",
    "            sum_co = s3.iloc[j]['sum']\n",
    "            sum_no = total - sum_co\n",
    "            ax.text(j, p_co/2, f\"{p_co:.1%}\\n({sum_co})\",  # :.0g\n",
    "                    ha='center', va='center', color='white', fontsize=8)\n",
    "            ax.text(j, p_co + p_no/2, f\"{p_no:.1%}\\n({sum_no})\",  # :.0g\n",
    "                    ha='center', va='center', color='white', fontsize=8)\n",
    "        ax.set_xticks(x_sub)\n",
    "        ax.set_xticklabels(cross_labels)\n",
    "        ax.set_title(nu_symbols[nu])\n",
    "        if ax == axes[0]:\n",
    "            ax.set_ylabel(\"Percentage\")\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=c, edgecolor='k') for c in ['blue', 'red']]\n",
    "    fig.legend(handles, ['Coexist', 'Non-coexist'], loc='upper right', bbox_to_anchor=(0.99, 0.99))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'img/hypothesis_3_{filter_option}.png', dpi=300)\n",
    "    plt.close()\n",
    "    # 4. Heatmap: Coexistence count/total and percentage\n",
    "    heat = df.groupby(['nu_sign', 'curve_cross']).agg(\n",
    "        total=('coexist', 'count'),\n",
    "        coexist=('coexist', 'sum')\n",
    "    ).reset_index().set_index(['nu_sign', 'curve_cross'])\n",
    "    # Prepare matrices\n",
    "    total_mat = heat['total'].unstack().reindex(index=nu_order, columns=[0, 1]).fillna(0)\n",
    "    co_mat = heat['coexist'].unstack().reindex(index=nu_order, columns=[0, 1]).fillna(0)\n",
    "    pct_mat = co_mat / total_mat.replace(0, np.nan)\n",
    "    # Create annotation text: coexist/total (pct%)\n",
    "    annot = (co_mat.astype(int).astype(str) + \"/\" + total_mat.astype(int).astype(str) + \n",
    "             \"\\n(\" + (pct_mat * 100).round(1).astype(str) + \"%)\").values\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        pct_mat,\n",
    "        annot=annot,\n",
    "        fmt='',\n",
    "        cmap=\"YlGnBu\",\n",
    "        cbar_kws={'label': 'Coexistence %'},\n",
    "        linewidths=0.5,\n",
    "        linecolor='grey',\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel(\"Curve Crossing\")\n",
    "    ax.set_ylabel(\"\\u03BD Sign\")\n",
    "    ax.set_xticklabels(cross_labels)\n",
    "    ax.set_yticklabels([nu_symbols[nu] for nu in nu_order], rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'img/coexistence_heatmap_{filter_option}.png', dpi=300)\n",
    "    plt.close()\n",
    "    # =====================================================================\n",
    "    # # 5. k Parameter Analysis Table\n",
    "    # k_analysis = df.groupby('k').agg(\n",
    "    #     total_cases=('k', 'count'),\n",
    "    #     cross_proportion=('curve_cross', 'mean'),\n",
    "    #     coexist_proportion=('coexist', 'mean')\n",
    "    # )\n",
    "    # print(\"\\n=== k Parameter Analysis ===\")\n",
    "    # print(k_analysis.to_string())\n",
    "    # =====================================================================\n",
    "    # 5. Core Analysis Tables\n",
    "    analysis_data = df.groupby(['nu_sign', 'curve_cross', 'coexist']).size().unstack().fillna(0)\n",
    "    analysis_data['Total'] = analysis_data.sum(axis=1)\n",
    "    analysis_data = analysis_data.reindex(pd.MultiIndex.from_product(\n",
    "        [nu_order, [0, 1]],\n",
    "        names=['\\u03BD Sign', 'Curve Cross']\n",
    "    ))\n",
    "    print(\"\\n=== Coexistence Analysis ===\")\n",
    "    print(analysis_data.astype(int).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_pipeline(filters, base_file, solver, truncate, extinc_crit_1):\n",
    "    os.makedirs('img', exist_ok=True)\n",
    "    os.makedirs('csv', exist_ok=True)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    if not os.path.exists(base_file):\n",
    "        print(\"Running simulation...\")\n",
    "        mesh = preprocess_data('table1')\n",
    "        results = [Sim(k, row, extinc_crit_1=extinc_crit_1, solver=solver) \n",
    "                   for k, row in tqdm(enumerate(mesh), total=len(mesh))]\n",
    "        postprocess_results(results, base_file)\n",
    "    for filter_option in filters:\n",
    "        filtered_filename = f\"csv/annplant_2spp_det_rare_filtered_{filter_option}.csv\"\n",
    "        if not os.path.exists(filtered_filename):\n",
    "            print(f\"\\nGenerating data for filter={filter_option}...\")\n",
    "            cor_figure(filter_option, truncate)\n",
    "        summary_path = f\"csv/pgr_analysis_summary_{filter_option}.csv\"\n",
    "        if not os.path.exists(summary_path):\n",
    "            plot_pgr_figures(filter_option, save_fig=False, extinc_crit_1=False)\n",
    "        try:\n",
    "            filtered_data = pd.read_csv(filtered_filename)\n",
    "            bool_cols = ['Coexist', 'A', 'B', 'C', 'D']\n",
    "            for col in bool_cols:\n",
    "                if col in filtered_data.columns:\n",
    "                    filtered_data[col] = filtered_data[col].astype(bool)\n",
    "            print(\"\\nAnalysis:\")\n",
    "            analyze_coexistence_effect(filtered_data, False)\n",
    "            plot_phase_plane()\n",
    "            count_abcd(filtered_data)\n",
    "            if 'C' in filtered_data.columns:\n",
    "                compare_counts_test(filtered_data[filtered_data['C']], True)\n",
    "        except Exception as e:\n",
    "            print(f\"Processing error: {str(e)}\")\n",
    "        analyze_coexistence_deterministic(filter_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    filters = ['on', 'off']\n",
    "    base_file = \"csv/annplant_2spp_det_rare.csv\"\n",
    "    solver = 'numeric' # 'analyN'\n",
    "    truncate = False\n",
    "    extinc_crit_1 = False\n",
    "    setup_pipeline(filters, base_file, solver, truncate, extinc_crit_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis:\n",
      "\n",
      "--- Analysis for SoS ---\n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                Coexist   No. Observations:                17823\n",
      "Model:                            GLM   Df Residuals:                    17819\n",
      "Model Family:                Binomial   Df Model:                            3\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -898.85\n",
      "Date:                Tue, 27 May 2025   Deviance:                       1797.7\n",
      "Time:                        15:56:17   Pearson chi2:                 3.69e+03\n",
      "No. Iterations:                    32   Pseudo R-squ. (CS):            0.06074\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       -242.4498   1.36e+05     -0.002      0.999   -2.67e+05    2.66e+05\n",
      "S1           229.6854   1.36e+05      0.002      0.999   -2.66e+05    2.67e+05\n",
      "FE1           18.3541      0.957     19.173      0.000      16.478      20.230\n",
      "cor_sos       -0.0019      0.000     -4.711      0.000      -0.003      -0.001\n",
      "==============================================================================\n",
      "\n",
      "Analysis on Negative ν for COR_SOS:\n",
      "Proportion of coexistence with ν ≥ 0: 0.98 (95% CI: (0.9807090670798713, 0.9846796905688473))\n",
      "Proportion of coexistence with ν < 0: 1 (95% CI: (0.9970649930174468, 1.0000000000000002))\n",
      "Coexistence and Exclusion based on ν for SoS:\n",
      "              ν ≥ 0  ν < 0\n",
      "Coexistence  16234   1305\n",
      "Exclusion      284      0\n",
      "Higher coexistence observed with ν < 0 for SoS, supporting the authors' results.\n",
      "A\n",
      "Coexist==0: 0\n",
      "Coexist==1: 0\n",
      "Total: 0\n",
      "Proportion: 0\n",
      "\n",
      "B\n",
      "Coexist==0: 284\n",
      "Coexist==1: 5710\n",
      "Total: 5994\n",
      "Proportion: 0.34\n",
      "\n",
      "C\n",
      "Coexist==0: 0\n",
      "Coexist==1: 11829\n",
      "Total: 11829\n",
      "Proportion: 0.66\n",
      "\n",
      "D\n",
      "Coexist==0: 0\n",
      "Coexist==1: 0\n",
      "Total: 0\n",
      "Proportion: 0\n",
      "\n",
      "PGR Statistics:\n",
      "\n",
      "PGR1 Mean ± SD: -3.3e-07 ± 6.2e-07\n",
      "PGR2 Mean ± SD: 2e-08 ± 1.5e-08\n",
      "\n",
      "Loaded 17823 valid records\n",
      "\n",
      "============================================================================================================================================\n",
      "Hypothesis Definitions:\n",
      "\n",
      "(H1) Sign of ν (cor_sos)\n",
      "\n",
      "(H2) Curve crossing (left/right edge dominance differs)\n",
      "\n",
      "(H3) PGR1 at left edge is higher\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Hypothesis Analysis Results ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|=====               | 9528/35646 [00:50<02:17]       "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
